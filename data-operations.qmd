---
title: "Data operations"
---


<!-- ##  Exploit evops app data pipelines -->


The evops app pipelines must be executed in the order of the following sections which are tagged as **Optional** (that is, can be skipped), **Mandatory** (that is, to run absolutely), **Recommended** (that is, can provide useful information to consider in the next pipeline) or **Independent** (that is, can be executed independently on previous pipelines).


:::{.callout-important}
Remember to upload all datasets to be processed by the evops app running container in the dedicated directory on the host computer specified when starting it. From now on, the input data are assumed to be in that directory unless specify.
:::


:::{.callout-note}
All two-dimensional datasets generated by any pipeline are automatically saved in the **workspace** directory on the evops app. Hence, these results are also available in the directory on the host computer dedicated to the evops app running container.
:::


:::{.callout-tip}
Before rerunning a pipeline with eventually modified input arguments on the same dataset of a given experimentation, be aware that all related results displayed in the notebook along with all folders and files created in the **workspace** directory during the previous run of the pipeline will be deleted or overwritten.
:::



##  Extract Sample Data (esd) pipeline: *Mandatory*


The notebook associated with **esd_pipeline** is the file **esd_pipeline.ipynb**.
The main objective of **esd_pipeline** is to extract or make a copy of a sample of size assigned to the variable **data_sample_size** from all data stored in the folder whose name is assigned to the variable **experimentation_dir** which contains the elements listed below. 

* **ground_truth_data**: It is a variable for the name of the csv file of ground truth data.

* **estimated_data**: It is a variable for the name of the csv file of estimated data.

* **error_data**: It is a variable for the name of the csv file of error data.

* **images_dir**: It is a variable for the name of the folder containing images.


Note that in order to make a copy of all data in the folder referenced by the variable **experimentation_dir**, it suffices to assign the value **-1** to the variable **data_sample_size**. The extracted datasets are stored in the folder whose name is made of **sample+experimentation_dir**. Its contents are organized like that of the folder indicated in the variable **experimentation_dir**.



```{mermaid}
flowchart LR
  I[Input] --> A(experimentation_dir)
  I --> B(data_sample_size)
  A --> A1(images_dir)
  A --> A2(ground_truth_data)
  A --> A3(estimated_data)
  A --> A4(error_data)
  A --> C{esd_pipeline}
  B --> C
  C --> O[output]
  O --> D(sample+experimentation_dir)
  D --> D1(images_dir)
  D --> D2(ground_truth_data)
  D --> D3(estimated_data)
  D --> D4(error_data)
```



##  Extract Transform Load (etl) pipeline: *Mandatory*


The notebook associated with **etl_pipeline** is the file **etl_pipeline.ipynb**.
The main objective of this pipeline is to construct the file **merged_data.csv**.  This file contains information on the instantaneous positions and velocities of the car sensors studied, the statistics of occurrences as well as the areas and locations associated with all classes of traffic scene objects. To produce the file **merged_data.csv**, the **etl_pipeline** requires two main information as input. The former is the folder whose name is assigned to the variable **experimentation_dir**. This folder must be generated by **esd_pipeline** since the running evops app container has all the privileges on it. It obviously contains the following elements. 

* **ground_truth_data**: It is a variable for the name of the csv file of ground truth data.

* **estimated_data**: It is a variable for the name of the csv file of estimated data.

* **error_data**: It is a variable for the name of the csv file of error data.

* **images_dir**: It is a variable for the name of the folder containing images.

The latter is a positive integer assigned to the variable **batch_size** which is the number of images to process at each iteration by the object detector to avoid a memory issue. 
The outputs are stored in the folder whose name is assigned to the variable **experimentation_dir**. This folder includes the following elements.

* **split_images_without_detections**: It is a folder in which all images from the folder whose name is assigned to the variable **images_dir** are partitioned. Images of the **k**-th partition are stored in the folder **split_k**.

* **split_images_with_detections**: It is a folder organized like  **split_images_without_detections** but contains annotated images by bounding boxes associated with all detected object classes.

* **split_images_detection_coordinates**: It a folder organized like **split_images_with_detections** but instead of annotated images, it contains the coordinates of the related bounding boxes in csv files.  

* **extracted_detections.csv**: It is a csv file which gathers the contents of all csv files stored in the folder **split_images_detection_coordinates**.

* **transformed_detections.csv**: It is a csv file which results from the transformation of the csv file **extracted_detections.csv** in order to synthetize it and provide more information on detections in some additional variables.


* **merged_data.csv**: It is the target file of **etl_pipeline**. This file contains not only variables from the file **transformed_detections.csv** and those from the file whose name is assigned to the variable **error_data**, but also the variables of velocity calculated using data from the files whose names are assigned to the variables **ground_truth_data** and **estimated_data**.




```{mermaid}
flowchart LR
 I[Input] --> A(experimentation_dir)
 I --> B(batch_size)
 A --> A1(images_dir)
 A --> A2(ground_truth_data)
 A --> A3(estimated_data)
 A --> A4(error_data)
 A --> C{etl_pipeline}
 B --> C
 C --> O[output]
 O --> D(experimentation_dir)
 D --> D3(split_images_without_detections)
 D --> D2(split_images_with_detections)
 D --> D1(split_images_detection_coordinates)
 D --> D4(extracted_detections.csv)
 D --> D5(transformed_detections.csv)
 D --> D6(merged_data.csv)
```




##  Outlier Data Extraction (ode) pipeline: *Optional*


The notebook associated with **ode_pipeline** is the file **ode_pipeline.ipynb**.
The main objective of this pipeline is to extract all outliers associated a given variable.
Before running the **ode_pipeline**, appropriate values must be assigned to the following input arguments.

* **coefficient_iqr**: It is a variable for the real value used to calculate the bounds of an interval out of which data are considered as outliers.

* **experimentation_dir**: It is a variable for the name of folder which contains both the csv file of errors whose name is assigned to the variable **error_data** and the folder of images whose name is assigned to the variable **images_dir**. 

* **response_var**: It is a variable for the name of the considered column in the file referenced by the variable **error_data**.

The outputs are stored in the folder whose name is assigned to the variable **experimentation_dir**. In this directory, images and data associated with the outliers are  stored in the sub folder whose name is assigned to the variable **response_var** of the folders **outliers_images** and **outliers_data**, respectively.  The name of the csv file containing outliers data is made of **outliers+response_var+csv**.




```{mermaid}
flowchart LR
  I[Input] --> A(experimentation_dir)
  A --> A1(error_data)
  A --> A2(images_dir)
  I[Input] --> B(response_var)
  I[Input] --> C(coefficient_iqr)
  A --> D{ode_pipeline}
  B --> D
  C --> D
  D --> O[output]
  O --> E(experimentation_dir)
  E --> F(outliers_images)
  F --> F1(response_var)
  F1 --> F2(images_files)
  E --> G(outliers_data)
  G --> H(response_var)
  H --> H1(outliers+response_var+csv)
```


```{mermaid}
flowchart LR
  I[Input] --> A(experimentation_dir)
  A --> A1(error_data)
  A --> A2(images_dir)
  I[Input] --> B(response_var)
  I[Input] --> C(coefficient_iqr)
  A --> D{ode_pipeline}
  B --> D
  C --> D
  D --> O[output]
```


```{mermaid}
flowchart LR
  O[output] --> E(experimentation_dir)
  E --> F(outliers_images)
  F --> F1(response_var)
  F1 --> F2(images_files)
  E --> G(outliers_data)
  G --> H(response_var)
  H --> H1(outliers+response_var+csv)
```




##  Comparative data analysis (cda) pipeline: *Optional*



The notebook associated with **cda_pipeline** is the file **cda_pipeline.ipynb**. The main objective of this pipeline is to provide some comparative graphics of response variables and predictor variables associated with both train and test datasets. 


To run the **cda_pipeline**, a file named **merged_data.csv** generated by **etl_pipeline** must be present in the directories referenced by the variables **experimentation_train_dir** and **experimentation_test_dir**. 

All outputs are displayed in the notebook and can be exported for reporting.
Numerical outputs are stored in the directory whose name is assigned to the variable **experimentation_train_dir**. This folder contains the five csv files whose names are listed below.

*  **longitudinal_error_summary_statistics.csv**: It is the file which contains summary statistics of the variable **longitudinal_error** associated with both train and test datasets.

*  **lateral_error_summary_statistics.csv**: It is the file which contains summary statistics of the variable **lateral_error** associated with both train and test datasets.

*  **velocity_summary_statistics.csv**: It is the file which contains summary statistics of the variable **velocity** associated with both train and test datasets.

*  **object_summary_statistics.csv**: It is the file which contains summary statistics of the variable **object** associated with both train and test datasets. This variable stands for the number of objects per image.

*  **area_summary_statistics.csv**: It is the file which contains summary statistics of the variable **area** associated with both train and test datasets. This variable stands for the normalized total area of objects per image.



```{mermaid}
flowchart LR
  I[Input] --> A(experimentation_train_dir)
  A --> A1(merged_data.csv)
  I[Input] --> B(experimentation_test_dir)
  B --> B1(merged_data.csv)
  A --> E{cda_pipeline}
  B --> E
  E --> O[output]
  O --> H(experimentation_train_dir)
  H --> H1(longitudinal_error_summary_statistics.csv)
  H --> H2(lateral_error_summary_statistics.csv)
  H --> H3(velocity_summary_statistics.csv)
  H --> H4(object_summary_statistics.csv)
  H --> H5(area_summary_statistics.csv)
```








##  Object Detection Statistics (ods) pipeline: *Recommended*


The notebook associated with **ods_pipeline** is the file **ods_pipeline.ipynb**.
The main objective of this pipeline is to extract statistics on the total number of detections associated with each class of objects. These statistics along with the related bar chart are displayed in the notebook. The detection statistics are also stored in the csv file named **detection_statistics.csv** in the directory whose name is assigned to the input argument **experimentation_dir**. The **ods_pipeline** depends on the **etl_pipeline** since its execution requires that the csv file **merged_data.csv** be present in the directory referenced by the variable **experimentation_dir**. This is why the name **merged_data.csv** is assigned by default to the input argument **merged_data**.



```{mermaid}
flowchart LR
  I[Input] --> A(experimentation_dir)
  A --> A1(merged_data)
  A --> B{ods_pipeline}
  B --> O[output]
  O --> C(experimentation_dir)
  C --> C1(detection_statistics.csv)
```




##  Impute Transform Load (itl) pipeline: *Mandatory*


:::{.callout-important}
Before running **itl_pipeline**, it is recommended to run **ods_pipeline** and explore the extracted detection statistics which are ordered from the most detected object class to the least detected object class. Then, define the number of object classes whose detection statistics are considered as large enough. That number is the value which must be assigned to the input variable **nlargest_classes** of the **itl_pipeline**.
:::



:::{.callout-note}
In general, the file **merged_data.csv** generated by **etl_pipeline** contains non standardized data and a significant number of misssing values due to the absence of some object classes in certain parts of the traffic scene. 
:::



The notebook associated with **itl_pipeline** is the file **itl_pipeline.ipynb**. The objectives of **itl_pipeline** are to impute the missing values in the file **merged_data.csv** and to standardize the values of its variables in order to generated data ready to feed statistical learning models.

To run the **itl_pipeline**, a file named **merged_data.csv** generated by **etl_pipeline** must be present in the directories referenced by the variables **experimentation_train_dir** and **experimentation_test_dir**. In addition, a file named **detection_statistics.csv** which is generated by **ods_pipeline** must be present in the directories referenced by the variables **experimentation_train_dir**. 

Besides, the studied response variable name must be assigned to the input argument **response_var** while the number of most detected object classes (identified when exploring the output generated by **ods_pipeline**)  must be assigned to the input argument **nlargest_classes**. Note that in order to use an optimal value of this parameter, it suffices to assign the value -1 to the variable **nlargest_classes**.

Outputs are stored in the sub folder **processed_data** of the directory whose name is assigned to the variable **experimentation_train_dir**.  In the directory **processed_data**, the sub folder whose name is the value assigned to the input argument **response_var** contains the four csv files conventionally named as follows.

*  **experimentation_train_dir+train+response_var+response.csv**: It is the file which contains the train response variable.

*  **experimentation_train_dir+train+response_var+scaled_covariates.csv**: It is the file which contains the train predictor variables.

*  **experimentation_test_dir+test+response_var+response.csv**: It is the file which contains the test response variable.

*  **experimentation_test_dir+test+response_var+scaled_covariates.csv**: It is the file which contains the test predictor variables.





```{mermaid}
flowchart LR
  I[Input] --> A(experimentation_train_dir)
  A --> A1(merged_data.csv)
  A --> A2(detection_statistics.csv)
  I[Input] --> B(experimentation_test_dir)
  B --> B1(merged_data.csv)
  I --> D(nlargest_classes)
  I --> C(response_var)
  A --> E{itl_pipeline}
  B --> E
  C --> E
  D --> E
  E --> O[output]
  O --> F(experimentation_train_dir)
  F --> G(processed_data)
  G --> H(response_var)
  H --> H1(experimentation_train_dir+train+response_var+response.csv)
  H --> H2(experimentation_train_dir+train+response_var+scaled_covariates.csv)
  H --> H3(experimentation_test_dir+test+response_var+response.csv)
  H --> H4(experimentation_test_dir+test+response_var+scaled_covariates.csv)
```



```{mermaid}
flowchart LR
  I[Input] --> A(experimentation_train_dir)
  A --> A1(merged_data.csv)
  A --> A2(detection_statistics.csv)
  I[Input] --> B(experimentation_test_dir)
  B --> B1(merged_data.csv)
  I --> D(nlargest_classes)
  I --> C(response_var)
  A --> E{itl_pipeline}
  B --> E
  C --> E
  D --> E
  E --> O[output]
```



```{mermaid}
flowchart LR
  O[output] --> F(experimentation_train_dir)
  F --> G(processed_data)
  G --> H(response_var)
  H --> H1(experimentation_train_dir+train+response_var+response.csv)
  H --> H2(experimentation_train_dir+train+response_var+scaled_covariates.csv)
  H --> H3(experimentation_test_dir+test+response_var+response.csv)
  H --> H4(experimentation_test_dir+test+response_var+scaled_covariates.csv)
```



