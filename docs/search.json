[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This guide is designed to serve your journey into the evops app which aims to extract knowledge from data using statistical models involving generalized extreme value (gev) distributions. By the end of this guide, you will have the knowledge to use evops app confidently. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of evops app.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "About",
    "section": "Install",
    "text": "Install\nThe evops app is packaged into a docker image which can be pulled from Docker Hub. A Docker platform is thus required to run evops app. Docker Desktop offers a GUI (Graphical User Interface) docker platform on a computer. It can be used to easily manage and run evops app in an isolated environment called a container. Initial setup can be performed by successively following the instructions provided in Install docker desktop and Install evops app.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "index.html#manage",
    "href": "index.html#manage",
    "title": "About",
    "section": "Manage",
    "text": "Manage\nThe startup and upgrade procedures are described in Manage.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "index.html#manipulate",
    "href": "index.html#manipulate",
    "title": "About",
    "section": "Manipulate",
    "text": "Manipulate\nThe evops app is made of pipelines which are implemented in notebooks. The common tasks related to the manipulation of these notebooks are presented in Manipulate.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "index.html#exploit",
    "href": "index.html#exploit",
    "title": "About",
    "section": "Exploit",
    "text": "Exploit\nMoreover, the evops app pipelines can be classified into two groups.\n\nThe former group is dedicated to data wrangling from raw data collected during an experimentation which aims to evaluate the impact of kinematic parameters and traffic scene object classes on car localization errors.\nThe latter group focuses on learning and exploiting two types of statistical models, namely arithmetic and geometric mixture of gev distributions.\n\n\nData operations\nThe usage of this data pipeline is explained in Data.\n\n\nModel operations\nThe user manual of this model pipeline is exposed in Model. It is important to note that the pipeline related to the geometric mixture of gev models can be executed on custom data while the pipeline related to the arithmetic mixture of gev models is designed to run only on pre-processed data from evops app data pipeline.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "setup-evops.html",
    "href": "setup-evops.html",
    "title": "Setup evops app",
    "section": "",
    "text": "To open Windows Terminal one can proceed as follows.\n\nSearch for terminal.\nSelect Invite de commandes in the search results.\n\n\n\n\nlisting_003\n\n\n\n\n\nlisting_004",
    "crumbs": [
      "Install",
      "Setup evops app"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "model-operations.html",
    "href": "model-operations.html",
    "title": "Model operations",
    "section": "",
    "text": "Note\n\n\n\nRerunning this pipeline with eventually modified input arguments on the same dataset of a given experimentation will overwrite all results generated during the previous run.\n\n\nThe notebook associated with sla_pipeline is the file sla_pipeline.ipynb. The main objective of this pipeline is to estimate the parameters of (stationary and non stationary) arithmetic mixture of gev models based on train datasets and to exploit the fitted model to predict return levels associated with given return period conditional on test predictor variables. Here, the train and test datasets must be those generated by itl_pipeline.\nTo run sla_pipeline, the studied response variable name must be assigned to the input argument response_var. In addition, the folder names containing raw train and test datasets must be assigned to the input arguments experimentation_train_dir and experimentation_test_dir. These three input arguments must match those used in itl_pipeline to create the considered train and test datasets. Note that the estimated models are automatically stored for future use since sla_pipeline can take a while to complete. Thus, a Boolean value must be assigned to the input argument use_pre_trained_models to indicate whether to load and exploit pre-trained models provided that they exist.\nAll numerical and graphical outputs are displayed in the notebook. Some numerical outputs are stored in the sub folder processed_data of the directory whose name is assigned to the variable experimentation_train_dir. In the directory processed_data, the sub folder whose name is the value assigned to the input argument response_var includes the csv files conventionally named as follows.\n\nexperimentation_train_dir+response_var+stationary_model_estimates.csv: It is the file which contains the estimated parameters of the stationary arithmetic gev mixture model fitted using only the train response variable.\nexperimentation_train_dir+response_var+non_stationary_model_estimates.csv: It is the file which contains the estimated parameters of the non stationary arithmetic gev mixture model fitted using both the train response variable and train predictor variables.\nexperimentation_train_dir+response_var+arithmetic_gev_mixture_model.rds: It is the file which contains all useful information about the estimated stationary arithmetic gev mixture model. This file, if it exists, is loaded for model exploitation when the parameter use_pre_trained_models is set to TRUE.\nexperimentation_train_dir+response_var+arithmetic_ns_gev_mixture_model.rds: It is the file which contains all useful information about the estimated non stationary arithmetic gev mixture model. This file, if it exists, is loaded for model exploitation when the parameter use_pre_trained_models is set to TRUE.\nexperimentation_train_dir+response_var+sample.csv: It is the file which contains a sample of the response variable generated using the fitted stationary arithmetic gev mixture model. The sample size can be modified as per needs. Its default value is equal to the number of observations in the train datasets.\nexperimentation_train_dir+response_var+importance.csv: It is the file which contains the contribution or importance of the predictor variables on the expected exceedance above the maximum value of the train response variable.\nexperimentation_train_dir+experimentation_test_dir+response_var+return_periods.csv: It is the file which contains return periods of observations in the test response variable conditional on their respective test predictor variables.\nexperimentation_train_dir+experimentation_test_dir+response_var+return_levels.csv: It is the file which contains return levels of the response variable conditional on the observed test predictor variables. The common return period associated with these return levels can be modified as per needs. Its default value is equal to the number of observations in the train datasets.\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(experimentation_train_dir)\n  I[Input] --&gt; B(experimentation_test_dir)\n  I[Input] --&gt; C(response_var)\n  I[Input] --&gt; C1(use_pre_trained_models)\n  A --&gt; D{sla_pipeline}\n  B --&gt; D\n  C --&gt; D\n  C1 --&gt; D\n  D --&gt; O[output]\n  O --&gt; E(experimentation_train_dir)\n  E --&gt; F(processed_data)\n  F --&gt; G(response_var)\n  G --&gt; G5(experimentation_train_dir+response_var+stationary_model_estimates.csv)\n  G --&gt; G6(experimentation_train_dir+response_var+non_stationary_model_estimates.csv)\n  G --&gt; G9(experimentation_train_dir+response_var+arithmetic_gev_mixture_model.rds)\n  G --&gt; G10(experimentation_train_dir+response_var+arithmetic_ns_gev_mixture_model.rds)\n  G --&gt; G3(experimentation_train_dir+response_var+sample.csv)\n  G --&gt; G4(experimentation_train_dir+response_var+importance.csv)\n  G --&gt; G7(experimentation_train_dir+experimentation_test_dir+response_var+return_periods.csv)\n  G --&gt; G8(experimentation_train_dir+experimentation_test_dir+response_var+return_levels.csv)\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(experimentation_train_dir)\n  I[Input] --&gt; B(experimentation_test_dir)\n  I[Input] --&gt; C(response_var)\n  I[Input] --&gt; C1(use_pre_trained_models)\n  A --&gt; D{sla_pipeline}\n  B --&gt; D\n  C --&gt; D\n  C1 --&gt; D\n  D --&gt; O[output]\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  O[output] --&gt; E(experimentation_train_dir)\n  E --&gt; F(processed_data)\n  F --&gt; G(response_var)\n  G --&gt; G5(experimentation_train_dir+response_var+stationary_model_estimates.csv)\n  G --&gt; G6(experimentation_train_dir+response_var+non_stationary_model_estimates.csv)\n  G --&gt; G9(experimentation_train_dir+response_var+arithmetic_gev_mixture_model.rds)\n  G --&gt; G10(experimentation_train_dir+response_var+arithmetic_ns_gev_mixture_model.rds)\n  G --&gt; G3(experimentation_train_dir+response_var+sample.csv)\n  G --&gt; G4(experimentation_train_dir+response_var+importance.csv)\n  G --&gt; G7(experimentation_train_dir+experimentation_test_dir+response_var+return_periods.csv)\n  G --&gt; G8(experimentation_train_dir+experimentation_test_dir+response_var+return_levels.csv)",
    "crumbs": [
      "Model",
      "Model operations"
    ]
  },
  {
    "objectID": "model-operations.html#sec-evasimuateddata",
    "href": "model-operations.html#sec-evasimuateddata",
    "title": "Model operations",
    "section": "Illustration on simulated datasets",
    "text": "Illustration on simulated datasets\n\n\n\n\n\n\nNote\n\n\n\nRerunning the pipelines of this section with modified input arguments will overwrite all results generated during the previous run.\n\n\n\nNormal Distribution\nThe notebook associated with eva_pipeline for data from a normal distribution is the file eva_pipeline_normal.ipynb. The main objective of this pipeline is to estimate the parameters of stationary geometric mixture of gev models based on simulated data from a normal probability distribution and to exploit the fitted model to predict return levels and expected exceedances associated with given return periods. In addition, the true return levels of the underlying normal law are provided for comparison.\nThe eva_pipeline for normal distribution requires three main information as input. The first is the number of normal data to simulate. This number must be assigned to the input argument n. The second and third are the mean and standard deviation parameters of the normal law to use. These two quantities must be assigned to the input arguments mean and sd, respectively.\nAll numerical and graphical outputs are displayed in the notebook. Some numerical outputs are stored in the normal_distribution_results directory as csv files conventionally named as follows.\n\nsample_normal_data.csv: It is the file which contains the simulated normal data using the provided input arguments.\ngev_mixture_model_estimates.csv: It is the file which contains the estimated parameters of the stationary geometric gev mixture model fitted using the simulated normal data.\nreturn_levels.csv: It is the file which contains estimated and true return levels of the normal law. The return periods associated with these return levels range from the number of observations in the simulated normal data to one million times that number. The used default values of return periods can be modified as per needs.\nexpected_exceedance.csv: It is the file which contains the estimated expected exceedances of a given threshold related to the input normal law for all gev models in the fitted geometric gev mixture model. The default value of that threshold is the maximum value of the simulated normal data. This value can be modified as per needs.\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(n)\n  I[Input] --&gt; B(mean)\n  I[Input] --&gt; C(sd)\n  A --&gt; D{eva_pipeline}\n  B --&gt; D\n  C --&gt; D\n  D --&gt; O[output]\n  O --&gt; E(normal_distribution_results)\n  E --&gt; E1(sample_normal_data.csv)\n  E --&gt; E2(gev_mixture_model_estimates.csv)\n  E --&gt; E3(return_levels.csv)\n  E --&gt; E4(expected_exceedance.csv)\n\n\n\n\n\n\n\n\nGamma Distribution\nThe notebook associated with eva_pipeline for data from a gamma distribution is the file eva_pipeline_gamma.ipynb. The main objective of this pipeline is to estimate the parameters of stationary geometric mixture of gev models based on simulated data from a gamma probability distribution and to exploit the fitted model to predict return levels and expected exceedances associated with given return periods. In addition, the true return levels of the underlying gamma law are provided for comparison.\nThe eva_pipeline for gamma distribution requires three main information as input. The first is the number of gamma data to simulate. This number must be assigned to the input argument n. The second and third are the shape and scale parameters of the gamma law to use. These two quantities must be assigned to the input arguments shape and scale, respectively.\nAll numerical and graphical outputs are displayed in the notebook. Some numerical outputs are stored in the gamma_distribution_results directory as csv files conventionally named as follows.\n\nsample_gamma_data.csv: It is the file which contains the simulated gamma data using the provided input arguments.\ngev_mixture_model_estimates.csv: It is the file which contains the estimated parameters of the stationary geometric gev mixture model fitted using the simulated gamma data.\nreturn_levels.csv: It is the file which contains estimated and true return levels of the gamma law. The return periods associated with these return levels range from the number of observations in the simulated gamma data to one million times that number. The used default values of return periods can be modified as per needs.\nexpected_exceedance.csv: It is the file which contains the estimated expected exceedances of a given threshold related to the input gamma law for all gev models in the fitted geometric gev mixture model. The default value of that threshold is the maximum value of the simulated gamma data. This value can be modified as per needs.\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(n)\n  I[Input] --&gt; B(shape)\n  I[Input] --&gt; C(scale)\n  A --&gt; D{eva_pipeline}\n  B --&gt; D\n  C --&gt; D\n  D --&gt; O[output]\n  O --&gt; E(gamma_distribution_results)\n  E --&gt; E1(sample_gamma_data.csv)\n  E --&gt; E2(gev_mixture_model_estimates.csv)\n  E --&gt; E3(return_levels.csv)\n  E --&gt; E4(expected_exceedance.csv)\n\n\n\n\n\n\n\n\nGEV Distribution\nThe notebook associated with eva_pipeline for data from a gev distribution is the file eva_pipeline_gev.ipynb. The main objective of this pipeline is to estimate the parameters of stationary geometric mixture of gev models based on simulated data from a gev probability distribution and to exploit the fitted model to predict return levels and expected exceedances associated with given return periods. In addition, the true return levels of the underlying gev law are provided for comparison.\nThe eva_pipeline for gev distribution requires four main information as input. The first is the number of gev data to simulate. This number must be assigned to the input argument n. The second, third and fourth are the shape, scale and location parameters of the gev law to use. These three quantities must be assigned to the input arguments shape, scale and loc, respectively.\nAll numerical and graphical outputs are displayed in the notebook. Some numerical outputs are stored in the gev_distribution_results directory as csv files conventionally named as follows.\n\nsample_gev_data.csv: It is the file which contains the simulated gev data using the provided input arguments.\ngev_mixture_model_estimates.csv: It is the file which contains the estimated parameters of the stationary geometric gev mixture model fitted using the simulated gev data.\nreturn_levels.csv: It is the file which contains estimated and true return levels of the gev law. The return periods associated with these return levels range from the number of observations in the simulated gev data to one million times that number. The used default values of return periods can be modified as per needs.\nexpected_exceedance.csv: It is the file which contains the estimated expected exceedances of a given threshold related to the input gev law for all gev models in the fitted geometric gev mixture model. The default value of that threshold is the maximum value of the simulated gev data. This value can be modified as per needs.\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(n)\n  I[Input] --&gt; B1(shape)\n  I[Input] --&gt; B2(scale)\n  I[Input] --&gt; B3(loc)\n  A --&gt; D{eva_pipeline}\n  B1 --&gt; D\n  B2 --&gt; D\n  B3 --&gt; D\n  D --&gt; O[output]\n  O --&gt; E(gev_distribution_results)\n  E --&gt; E1(sample_gev_data.csv)\n  E --&gt; E2(gev_mixture_model_estimates.csv)\n  E --&gt; E3(return_levels.csv)\n  E --&gt; E4(expected_exceedance.csv)",
    "crumbs": [
      "Model",
      "Model operations"
    ]
  },
  {
    "objectID": "model-operations.html#sec-evarealdata",
    "href": "model-operations.html#sec-evarealdata",
    "title": "Model operations",
    "section": "Application on real dataset",
    "text": "Application on real dataset\n\n\n\n\n\n\nNote\n\n\n\nRerunning this pipeline with modified input arguments on a given uploaded dataset will overwrite all results generated during the previous run.\n\n\nThe notebook associated with eva_pipeline is the file eva_pipeline.ipynb. The main objective of this pipeline is to estimate the parameters of (stationary) geometric mixture of gev models based on train data and to exploit the fitted model to predict return levels and expected exceedances associated with given return periods.\nThe eva_pipeline requires two main information as input. The former is the csv file whose name is assigned to the input argument csv_file_name. This csv file must be uploaded in the workspace directory of the container running the evops app. The latter concerns the column name of the variable to consider in the file referenced by the argument csv_file_name. This column name must be assigned to the input argument variable_name.\nAll numerical and graphical outputs are displayed in the notebook. Some numerical outputs are stored in the workspace directory of the container running the evops app as csv files conventionally named as follows.\n\ncsv_file_name+variable_name+gev_mixture_model_estimates.csv: It is the file which contains the estimated parameters of the stationary geometric gev mixture model fitted using the specified train data.\ncsv_file_name+variable_name+return_levels.csv: It is the file which contains return levels of the fitted variable. The return periods associated with these return levels range from the number of observations in the train data to one million times that number. The used default values of return periods can be modified as per needs.\ncsv_file_name+variable_name+expected_exceedance.csv: It is the file which contains the expected exceedances of a given threshold related to the fitted variable for all gev models in the fitted geometric gev mixture model. The default value of that threshold is the maximum value of the train data. This value can be modified as per needs.\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(csv_file_name)\n  I[Input] --&gt; B(variable_name)\n  A --&gt; C{eva_pipeline}\n  B --&gt; C\n  C --&gt; O[output]\n  O --&gt; D(workspace)\n  D --&gt; D1(csv_file_name+variable_name+gev_mixture_model_estimates.csv)\n  D --&gt; D2(csv_file_name+variable_name+return_levels.csv)\n  D --&gt; D3(csv_file_name+variable_name+expected_exceedance.csv)",
    "crumbs": [
      "Model",
      "Model operations"
    ]
  },
  {
    "objectID": "manipulate-evops.html",
    "href": "manipulate-evops.html",
    "title": "Manipulate evops app",
    "section": "",
    "text": "To display the list of all available pipelines, one can proceed as follows.\n\nClick to hide or show the file browser.\nClick to display the folders of available pipelines and directories.\n\n\n\n\nlisting_024",
    "crumbs": [
      "Manipulate",
      "Manipulate evops app"
    ]
  },
  {
    "objectID": "manage-evops.html",
    "href": "manage-evops.html",
    "title": "Manage evops app",
    "section": "",
    "text": "Important\n\n\n\nBefore starting for the first time, create a folder in your computer which is dedicated to the evops app data in the host computer. To illustrate, we have created a folder called workdir at the location:\nC:\\Users\\HEWLETT-PACKARD\\Documents\\workdir\n\n\nThe docker image of the evops app can be run directly from Docker Desktop. Open a Docker Desktop and follows the 13 steps outlined in this walkthrough illustrated in the screenshots below.\n\nClick on the Images tab.\nSearch for the image called dkengne/evops tagged as latest.\nClick on the play button related to the found image.\nExpand the Optional settings.\nIn the Host path, click on the horizontal dotted button.\nFind and select the folder (eg. workdir) dedicated to the evops app data in the host computer.\nClick on Selectionner un dossier to validate your selection.\nIn the Container name, enter a desired name. For instance: evops_container.\nIn the first Host port, specify 8787 which is the port of Rstudio Server.\nIn the second Host port, specify 8888 which is the port of JupyterLab Server.\nIn the Container path, enter /home/rstudio/workspace which is the directory dedicated to the evops app data within the container.\nSelect the Run button. That’s it! The container is ready to use.\nClick on the second link to open the evops app in a web browser.\n\n\n\n\nlisting_006\n\n\n\n\n\nlisting_007\n\n\n\n\n\nlisting_008\n\n\n\n\n\nlisting_009\n\n\n\n\n\nlisting_010\n\n\n\n\n\nlisting_011",
    "crumbs": [
      "Manage",
      "Manage evops app"
    ]
  },
  {
    "objectID": "manage-evops.html#from-docker-desktop",
    "href": "manage-evops.html#from-docker-desktop",
    "title": "Manage evops app",
    "section": "From Docker Desktop",
    "text": "From Docker Desktop\nTo stop a running docker container from Docker Desktop, one can proceed as follows.\n\nClick on the Containers tab.\nSearch for the name of the running container to stop.\nClick on the stop button related to the found container.\n\n\n\n\nlisting_012",
    "crumbs": [
      "Manage",
      "Manage evops app"
    ]
  },
  {
    "objectID": "manage-evops.html#from-web-browser",
    "href": "manage-evops.html#from-web-browser",
    "title": "Manage evops app",
    "section": "From Web Browser",
    "text": "From Web Browser\nTo stop evops app which is open in a Web Browser, one can proceed as follows.\n\nClick on the File tab.\nSelect Shut Down in the scrolling menu.\nClick the X in the top-right corner of the JupyterLab tab.\n\n\n\n\nlisting_013",
    "crumbs": [
      "Manage",
      "Manage evops app"
    ]
  },
  {
    "objectID": "data-operations.html",
    "href": "data-operations.html",
    "title": "Data operations",
    "section": "",
    "text": "The evops app pipelines must be executed in the order of the following sections which are tagged as Optional (that is, can be skipped), Mandatory (that is, to run absolutely), Recommended (that is, can provide useful information to consider in the next pipeline) or Independent (that is, can be executed independently on previous pipelines).",
    "crumbs": [
      "Data",
      "Data operations"
    ]
  },
  {
    "objectID": "statistical-models.html",
    "href": "statistical-models.html",
    "title": "Statistical models",
    "section": "",
    "text": "The extreme value model to estimate is the arithmetic mixture of generalized extreme value (GEV) distribution \\(G_{\\mathbb{S}}\\) defined by\n\\[\nG_{\\mathbb{S}}(x;\\omega) = \\sum_{j = 1}^{p} \\omega_j\\, G_{\\mathbb{S}, j}(x),\n\\] where \\(x \\in \\mathbb{R};\\) \\(\\omega = \\left(\\omega_1, \\cdots, \\omega_p \\right);\\) \\(0 &lt; \\omega_j \\leq 1;\\) \\(\\sum_{j = 1}^{p}\\omega_j = 1\\) and \\(G_{\\mathbb{S}, j}\\) is a GEV distribution with parameters \\(\\left(\\gamma_{\\mathbb{S}, j},\\sigma_{\\mathbb{S}, j},\\mu_{\\mathbb{S}, j}\\right)\\in \\mathbb{R}^3.\\)\n\n\n\nLet \\(y = (y_1, \\cdots, y_q)\\in \\mathbb{R}^q\\) be a vector of \\(q\\) predictor variables. The extreme value model to estimate is the arithmetic mixture of generalized extreme value (GEV) distribution \\(G_{\\mathbb{S}}(\\cdot|y)\\) defined by\n\\[\nG_{\\mathbb{S}}(x;\\omega|y) = \\sum_{j = 1}^{p} \\omega_j\\, G_{\\mathbb{S}, j}(x|y),\n\\] where \\(x \\in \\mathbb{R};\\) \\(\\omega = \\left(\\omega_1, \\cdots, \\omega_p \\right);\\) \\(0 &lt; \\omega_j \\leq 1;\\) \\(\\sum_{j = 1}^{p}\\omega_j = 1\\) and \\(G_{\\mathbb{S}, j}(\\cdot|y)\\) is a GEV distribution with parameters \\(\\left(\\gamma_{\\mathbb{S}, j},\\sigma_{\\mathbb{S}, j}(y),\\mu_{\\mathbb{S}, j}(y)\\right)\\in \\mathbb{R}^3.\\)\nwith\n\\[\\mu_{\\mathbb{S}, j}(y) = \\mu_{\\mathbb{S},j, 0} + \\sum_{\\ell = 1}^{q} \\mu_{\\mathbb{S},j,\\ell}\\, y_{\\ell}\\]\nand\n\\[\\sigma_{\\mathbb{S}, j}(y) = \\sigma_{\\mathbb{S},j, 0} + \\sum_{\\ell = 1}^{q} \\sigma_{\\mathbb{S},j,\\ell}\\, y_{\\ell}.\\]",
    "crumbs": [
      "Model",
      "Statistical models"
    ]
  },
  {
    "objectID": "statistical-models.html#stationary-model",
    "href": "statistical-models.html#stationary-model",
    "title": "Statistical models",
    "section": "",
    "text": "The extreme value model to estimate is the arithmetic mixture of generalized extreme value (GEV) distribution \\(G_{\\mathbb{S}}\\) defined by\n\\[\nG_{\\mathbb{S}}(x;\\omega) = \\sum_{j = 1}^{p} \\omega_j\\, G^{\\omega_j}_{\\mathbb{S}, j}(x),\n\\] where \\(x \\in \\mathbb{R};\\) \\(\\omega = \\left(\\omega_1, \\cdots, \\omega_p \\right);\\) \\(0 &lt; \\omega_j \\leq 1;\\) \\(\\sum_{j = 1}^{p}\\omega_j = 1\\) and \\(G_{\\mathbb{S}, j}\\) is a GEV distribution with parameters \\(\\left(\\gamma_{\\mathbb{S}, j},\\sigma_{\\mathbb{S}, j},\\mu_{\\mathbb{S}, j}\\right)\\in \\mathbb{R}^3.\\)",
    "crumbs": [
      "Model",
      "Statistical models"
    ]
  },
  {
    "objectID": "statistical-models.html#non-stationary-model",
    "href": "statistical-models.html#non-stationary-model",
    "title": "Statistical models",
    "section": "",
    "text": "Let \\(y = (y_1, \\cdots, y_q)\\in \\mathbb{R}^q\\) be a vector of \\(q\\) predictor variables. The extreme value model to estimate is the arithmetic mixture of generalized extreme value (GEV) distribution \\(G_{\\mathbb{S}}(\\cdot|y)\\) defined by\n\\[\nG_{\\mathbb{S}}(x;\\omega|y) = \\sum_{j = 1}^{p} \\omega_j\\, G^{\\omega_j}_{\\mathbb{S}, j}(x|y),\n\\] where \\(x \\in \\mathbb{R};\\) \\(\\omega = \\left(\\omega_1, \\cdots, \\omega_p \\right);\\) \\(0 &lt; \\omega_j \\leq 1;\\) \\(\\sum_{j = 1}^{p}\\omega_j = 1\\) and \\(G_{\\mathbb{S}, j}(\\cdot|y)\\) is a GEV distribution with parameters \\(\\left(\\gamma_{\\mathbb{S}, j},\\sigma_{\\mathbb{S}, j}(y),\\mu_{\\mathbb{S}, j}(y)\\right)\\in \\mathbb{R}^3.\\)\nwith\n\\[\\mu_{\\mathbb{S}, j}(y) = \\mu_{\\mathbb{S},j, 0} + \\sum_{\\ell = 1}^{q} \\mu_{\\mathbb{S},j,\\ell}\\, y_{\\ell}\\]\nand\n\\[\\sigma_{\\mathbb{S}, j}(y) = \\sigma_{\\mathbb{S},j, 0} + \\sum_{\\ell = 1}^{q} \\sigma_{\\mathbb{S},j,\\ell}\\, y_{\\ell}.\\]",
    "crumbs": [
      "Model",
      "Statistical models"
    ]
  },
  {
    "objectID": "setup-docker.html",
    "href": "setup-docker.html",
    "title": "Setup docker desktop",
    "section": "",
    "text": "This page contains the download URL and instructions on how to install and configure Docker Desktop for Windows. Docker only supports Docker Desktop on Windows 10, Windows 11 Professional or Enterprise edition and Windows Home or Education editions.",
    "crumbs": [
      "Install",
      "Setup docker desktop"
    ]
  },
  {
    "objectID": "index.html#extreme-values-operations-evops-app",
    "href": "index.html#extreme-values-operations-evops-app",
    "title": "About",
    "section": "",
    "text": "This guide is designed to serve your journey into the evops app which aims to extract knowledge from data using statistical models involving generalized extreme value (gev) distributions. By the end of this guide, you will have the knowledge to use evops app confidently. In each of the sections below, you will find overviews, guides, and step-by-step tutorials to walk you through the features of evops app.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "index.html#get-started-with-evops-app",
    "href": "index.html#get-started-with-evops-app",
    "title": "About",
    "section": "Get started with evops app",
    "text": "Get started with evops app\n\nInstall\nThe evops app is packaged into a docker image which can be pulled from Docker Hub. A Docker platform is thus required to run evops app. Docker Desktop offers a GUI (Graphical User Interface) docker platform on a computer. It can be used to easily manage and run evops app in an isolated environment called a container. Initial setup can be performed by successively following the instructions provided in Install docker desktop and Install evops app.\n\n\nManage\nThe startup and upgrade procedures are described in Manage.\n\n\nManipulate\nThe evops app is made of pipelines which are implemented in notebooks. The common tasks related to the manipulation of these notebooks are presented in Manipulate.\n\n\nExploit\nMoreover, the evops app pipelines can be classified into two groups.\n\nThe former group is dedicated to data wrangling from raw data collected during an experimentation which aims to evaluate the impact of kinematic parameters and traffic scene object classes on car localization errors.\nThe latter group focuses on learning and exploiting two types of statistical models, namely arithmetic and geometric mixture of gev distributions. The mentioned arithmetic mixture model is fitted to the entire data and is used for short-term forecasting whereas the geometric mixture model focuses on extreme values and is used for long-term forecasting.\n\n\nData operations\nThe usage of data wrangling pipelines is explained in Data.\n\n\nModel operations\nThe user manual of model pipelines is exposed in Model. It is important to note that the pipeline related to the geometric mixture of gev models can be executed on custom data while the pipeline related to the arithmetic mixture of gev models is designed to run only on pre-processed data from evops app data pipelines.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "setup-docker.html#install-docker-desktop",
    "href": "setup-docker.html#install-docker-desktop",
    "title": "Setup docker desktop",
    "section": "Install Docker Desktop",
    "text": "Install Docker Desktop\nTo install Docker Desktop on Windows, a system administrator can follow the instructions below.\n\nDownload the .exe installer file using the following link https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe\nDouble-click on the downloaded .exe installer file.\nWhen prompted, ensure the Use WSL 2 instead of Hyper-V option on the Configuration page is selected.\nFollow the instructions on the installation wizard to authorize the installer and proceed with the install.\nWhen the installation is successful, select Close to complete the installation process.",
    "crumbs": [
      "Install",
      "Setup docker desktop"
    ]
  },
  {
    "objectID": "setup-docker.html#configure-docker-desktop",
    "href": "setup-docker.html#configure-docker-desktop",
    "title": "Setup docker desktop",
    "section": "Configure Docker Desktop",
    "text": "Configure Docker Desktop\nA computer user having standard account named, say username, can run docker desktop without administrator privileges. For that, it is suffices to add the account username to the docker-users group. This operation can be performed by a system administrator as follows.\n\nSearch for powershell.\nSelect Windows PowerShell in the search results.\nSelect Exécuter en tant que administrateur to open Windows PowerShell as administrator.\nCopy and paste the command below in the opened PowerShell. Then, hit the Enter key on the keyboard to display list of all computer user accounts.\n\nget-localuser\n\nLocate the account named username under the Name column of the displayed output. For instance, in the second screenshot below, the term username refers to HEWLETT-PACKARD.\nCopy and paste the command below in the opened PowerShell after replacing username with the appropriate value. Then, hit the Enter key on the keyboard to add the account username to the docker-users group.\n\nnet localgroup docker-users username /add\n\nRestart your computer for the changes to take effect and open Docker Desktop without administrator privileges.\n\n\n\n\nlisting_043\n\n\n\n\n\nlisting_044",
    "crumbs": [
      "Install",
      "Setup docker desktop"
    ]
  },
  {
    "objectID": "setup-docker.html#start-docker-desktop",
    "href": "setup-docker.html#start-docker-desktop",
    "title": "Setup docker desktop",
    "section": "Start Docker Desktop",
    "text": "Start Docker Desktop\nDocker Desktop does not start automatically after installation. To start Docker Desktop, one can procced as follows.\n\nSearch for Docker.\nselect Docker Desktop in the search results.\n\n\n\n\n\n\n\nNote\n\n\n\nFor the first start, the Docker menu displays the Docker Subscription Service Agreement. Select Accept to continue. Docker Desktop starts after you accept the terms. Note that Docker Desktop won’t run if you do not agree to the terms.\n\n\n\n\n\nlisting_001\n\n\n\n\n\nlisting_002",
    "crumbs": [
      "Install",
      "Setup docker desktop"
    ]
  },
  {
    "objectID": "setup-evops.html#open-windows-terminal",
    "href": "setup-evops.html#open-windows-terminal",
    "title": "Setup evops app",
    "section": "",
    "text": "To open Windows Terminal one can proceed as follows.\n\nSearch for terminal.\nSelect Invite de commandes in the search results.\n\n\n\n\nlisting_003\n\n\n\n\n\nlisting_004",
    "crumbs": [
      "Install",
      "Setup evops app"
    ]
  },
  {
    "objectID": "setup-evops.html#download-evops-app-image",
    "href": "setup-evops.html#download-evops-app-image",
    "title": "Setup evops app",
    "section": "Download evops app image",
    "text": "Download evops app image\nTo download the latest version of the docker image associated with the extreme value operations (evops) apps, one can proceed as follows after starting Docker Desktop.\n\nOpen a Windows Terminal.\nCopy and paste the following docker command in the Terminal:\n\ndocker pull dkengne/evops:latest\n\nHit the Enter key on the keyboard to start. For the first time, this may take a while.\n\n\n\n\nlisting_005",
    "crumbs": [
      "Install",
      "Setup evops app"
    ]
  },
  {
    "objectID": "manage-evops.html#start-evops-app",
    "href": "manage-evops.html#start-evops-app",
    "title": "Manage evops app",
    "section": "",
    "text": "Important\n\n\n\nBefore starting for the first time, create a folder in your computer which is dedicated to the evops app data in the host computer. To illustrate, we have created a folder called workdir at the location:\nC:\\Users\\HEWLETT-PACKARD\\Documents\\workdir\n\n\nThe docker image of the evops app can be run directly from Docker Desktop. Open a Docker Desktop and follows the 13 steps outlined in this walkthrough illustrated in the screenshots below.\n\nClick on the Images tab.\nSearch for the image called dkengne/evops tagged as latest.\nClick on the play button related to the found image.\nExpand the Optional settings.\nIn the Host path, click on the horizontal dotted button.\nFind and select the folder (eg. workdir) dedicated to the evops app data in the host computer.\nClick on Selectionner un dossier to validate your selection.\nIn the Container name, enter a desired name. For instance: evops_container.\nIn the first Host port, specify 8787 which is the port of Rstudio Server.\nIn the second Host port, specify 8888 which is the port of JupyterLab Server.\nIn the Container path, enter /home/rstudio/workspace which is the directory dedicated to the evops app data within the container.\nSelect the Run button. That’s it! The container is ready to use.\nClick on the second link to open the evops app in a web browser.\n\n\n\n\nlisting_006\n\n\n\n\n\nlisting_007\n\n\n\n\n\nlisting_008\n\n\n\n\n\nlisting_009\n\n\n\n\n\nlisting_010\n\n\n\n\n\nlisting_011",
    "crumbs": [
      "Manage",
      "Manage evops app"
    ]
  },
  {
    "objectID": "manage-evops.html#stop-evops-app",
    "href": "manage-evops.html#stop-evops-app",
    "title": "Manage evops app",
    "section": "Stop evops app",
    "text": "Stop evops app\nA docker container in which the evops app is running can be stopped either from the Docker Desktop or from the Web Browser.\n\nFrom Docker Desktop\nTo stop a running docker container from Docker Desktop, one can proceed as follows.\n\nClick on the Containers tab.\nSearch for the name of the running container to stop.\nClick on the stop button related to the found container.\n\n\n\n\nlisting_012\n\n\n\n\nFrom Web Browser\nTo stop evops app which is open in a Web Browser, one can proceed as follows.\n\nClick on the File tab.\nSelect Shut Down in the scrolling menu.\nClick the X in the top-right corner of the JupyterLab tab.\n\n\n\n\nlisting_013",
    "crumbs": [
      "Manage",
      "Manage evops app"
    ]
  },
  {
    "objectID": "manage-evops.html#restart-evops-app",
    "href": "manage-evops.html#restart-evops-app",
    "title": "Manage evops app",
    "section": "Restart evops app",
    "text": "Restart evops app\nTo restart a stopped docker container associated with evops app, one can proceed as follows.\n\nClick on the Containers tab.\nSearch for the name of the stopped container to restart.\nClick on the play button related to the found container.\nClick on the vertical dotted button right to the play button.\nSelect View details in the scrolling menu.\nClick on the second link to open the evops app in a web browser.\n\n\n\n\nlisting_014\n\n\n\n\n\nlisting_015\n\n\n\n\n\nlisting_016",
    "crumbs": [
      "Manage",
      "Manage evops app"
    ]
  },
  {
    "objectID": "manage-evops.html#view-resource-utilization",
    "href": "manage-evops.html#view-resource-utilization",
    "title": "Manage evops app",
    "section": "View resource utilization",
    "text": "View resource utilization\nTo view resource utilization of a running container from Docker Desktop, one can proceed as follows.\n\nClick on the Containers tab to display the list of running or stopped containers.\nSearch for the running container concerned by resource consumption.\nClick on the vertical dotted button right to the stop button.\nSelect View details in the scrolling menu.\nClick on the Stats tab to display resource utilization charts of the selected container.\n\n\n\n\nlisting_017\n\n\n\n\n\nlisting_018",
    "crumbs": [
      "Manage",
      "Manage evops app"
    ]
  },
  {
    "objectID": "manage-evops.html#delete-evops-app-container",
    "href": "manage-evops.html#delete-evops-app-container",
    "title": "Manage evops app",
    "section": "Delete evops app container",
    "text": "Delete evops app container\nTo delete a created docker container from Docker Desktop, one can proceed as follows.\n\nClick on the Containers tab to display the list of running or stopped containers.\nCheck the box left to the container name to delete.\nClick on the Delete button.\nClick on the Delete forever button to confirm deletion.\n\n\n\n\nlisting_019\n\n\n\n\n\nlisting_020",
    "crumbs": [
      "Manage",
      "Manage evops app"
    ]
  },
  {
    "objectID": "manage-evops.html#update-evops-app-image",
    "href": "manage-evops.html#update-evops-app-image",
    "title": "Manage evops app",
    "section": "Update evops app image",
    "text": "Update evops app image\nBefore the update of a docker image associated with the evops app, delete all docker containers (running or stopped) using the image to update. After that, one can follow the steps described below to update the latest version of docker image called dkengne/evops from Docker Desktop.\n\nClick on the Images tab to display the list of in use or unused images.\nSearch for the image called dkengne/evops and tagged as latest.\nClick on the vertical dotted button left to the trash icon related to the found image.\nSelect Pull in the scrolling menu to start update.\nCheck the box left to the old image called dkengne/evops and tagged as &lt;none&gt;.\nClick on the Delete button.\nClick on the Delete forever button to confirm deletion.\n\n\n\n\nlisting_021\n\n\n\n\n\nlisting_022\n\n\n\n\n\nlisting_023",
    "crumbs": [
      "Manage",
      "Manage evops app"
    ]
  },
  {
    "objectID": "manipulate-evops.html#show-list-of-pipelines",
    "href": "manipulate-evops.html#show-list-of-pipelines",
    "title": "Manipulate evops app",
    "section": "",
    "text": "To display the list of all available pipelines, one can proceed as follows.\n\nClick to hide or show the file browser.\nClick to display the folders of available pipelines and directories.\n\n\n\n\nlisting_024",
    "crumbs": [
      "Manipulate",
      "Manipulate evops app"
    ]
  },
  {
    "objectID": "manipulate-evops.html#open-a-pipeline-notebook",
    "href": "manipulate-evops.html#open-a-pipeline-notebook",
    "title": "Manipulate evops app",
    "section": "Open a pipeline notebook",
    "text": "Open a pipeline notebook\nTo open a pipeline notebook, one can proceed as follows from the list of pipelines.\n\nDouble click on the name of the desired pipeline folder name to access its content.\nDouble click on the appropriate pipeline file name with extension .ipynb to display its contents in the main panel.\n\n\n\n\nlisting_025\n\n\n\n\n\nlisting_026",
    "crumbs": [
      "Manipulate",
      "Manipulate evops app"
    ]
  },
  {
    "objectID": "manipulate-evops.html#close-a-pipeline-notebook",
    "href": "manipulate-evops.html#close-a-pipeline-notebook",
    "title": "Manipulate evops app",
    "section": "Close a pipeline notebook",
    "text": "Close a pipeline notebook\nTo close an open pipeline notebook, one can proceed as follows.\n\nClick on the considered open pipeline notebook tab in the main panel.\nClick the X in the top-right corner of the selected notebook tab.\n\n\n\n\nlisting_030\n\n\nAn alternative procedure to close an open pipeline notebook consists to proceed as follows.\n\nRight-click on the considered open pipeline notebook tab in the main panel.\nSelect Close Tab in the scrolling menu.\n\n\n\n\nlisting_031",
    "crumbs": [
      "Manipulate",
      "Manipulate evops app"
    ]
  },
  {
    "objectID": "manipulate-evops.html#show-pipeline-table-of-contents",
    "href": "manipulate-evops.html#show-pipeline-table-of-contents",
    "title": "Manipulate evops app",
    "section": "show pipeline table of contents",
    "text": "show pipeline table of contents\nTo show the table of contents of an open pipeline notebook, one can proceed as follows.\n\nClick on the considered open pipeline notebook tab in the main panel.\nClick to hide or show the table of contents associated with the selected notebook.\n\n\n\n\nlisting_027",
    "crumbs": [
      "Manipulate",
      "Manipulate evops app"
    ]
  },
  {
    "objectID": "manipulate-evops.html#run-all-pipeline-cells",
    "href": "manipulate-evops.html#run-all-pipeline-cells",
    "title": "Manipulate evops app",
    "section": "Run all pipeline cells",
    "text": "Run all pipeline cells\nTo automatically run all cells of an open pipeline notebook, one can proceed as follows.\n\nClick on the considered open pipeline notebook tab in the main panel.\nEnter appropriate information in the input argument section.\nClick on the icon of the double play button to execute all stages of the pipeline.\n\n\n\n\nlisting_028",
    "crumbs": [
      "Manipulate",
      "Manipulate evops app"
    ]
  },
  {
    "objectID": "manipulate-evops.html#run-an-individual-pipeline-cell",
    "href": "manipulate-evops.html#run-an-individual-pipeline-cell",
    "title": "Manipulate evops app",
    "section": "Run an individual pipeline cell",
    "text": "Run an individual pipeline cell\n\n\n\n\n\n\nImportant\n\n\n\nCells in the notebook must be executed sequentially, because an individual cell in the notebook may depend on some previous cells.\n\n\nTo run a specific cell of an open pipeline notebook, one can proceed as follows.\n\nClick on the considered open pipeline notebook tab in the main panel.\nClick on the notebook cell to execute.\nClick on the icon of the play button to execute all codes in the selected cell.\n\n\n\n\nlisting_029",
    "crumbs": [
      "Manipulate",
      "Manipulate evops app"
    ]
  },
  {
    "objectID": "manipulate-evops.html#export-all-pipeline-graphics",
    "href": "manipulate-evops.html#export-all-pipeline-graphics",
    "title": "Manipulate evops app",
    "section": "Export all pipeline graphics",
    "text": "Export all pipeline graphics\nTo export all graphics generated in an open notebook, one can proceed as follows.\n\nClick on the considered open pipeline notebook tab in the main panel.\nClick on the File tab.\nSelect Save and Export Notebook As in the scrolling menu.\nSelect Markdown in the scrolling menu.\nMove on to the Téléchargements folder in the host computer.\nRight-click on the zipped file with the same base name as the considered notebook.\nSelect Extraire tout in the scrolling menu.\nSpecify the path to store the extracted files.\nClick on the Extraire button to extract files.\nOpen the folder containing the extracted files to access the graphics.\n\n\n\n\nlisting_032\n\n\n\n\n\nlisting_033\n\n\n\n\n\nlisting_034\n\n\n\n\n\nlisting_034\n\n\n\n\n\nlisting_036\n\n\n\n\n\nlisting_037\n\n\n\n\n\nlisting_038",
    "crumbs": [
      "Manipulate",
      "Manipulate evops app"
    ]
  },
  {
    "objectID": "manipulate-evops.html#upload-a-file-to-evops-app",
    "href": "manipulate-evops.html#upload-a-file-to-evops-app",
    "title": "Manipulate evops app",
    "section": "Upload a file to evops app",
    "text": "Upload a file to evops app\nTo upload a csv file to a running evops app, one can proceed as follows.\n\nClick to hide or show the file browser.\nClick to display the folders of available pipelines and directories.\nDouble-click on the workspace directory to open it.\nClick on the Upload icon to open a file browser in the host computer.\nBrowse the host computer to find and select the csv file to upload.\nClick on the Open button to upload the selected file.\n\n\n\n\n\n\n\nNote\n\n\n\nThe csv file uploaded by the above procedure is available in workspace directory of the running evops app.\n\n\n\n\n\nlisting_039\n\n\n\n\n\nlisting_040\n\n\n\n\n\nlisting_041\n\n\n\n\n\nlisting_042",
    "crumbs": [
      "Manipulate",
      "Manipulate evops app"
    ]
  },
  {
    "objectID": "data-operations.html#extract-sample-data-esd-pipeline-optional",
    "href": "data-operations.html#extract-sample-data-esd-pipeline-optional",
    "title": "Data operations",
    "section": "Extract Sample Data (esd) pipeline: Optional",
    "text": "Extract Sample Data (esd) pipeline: Optional\nThe notebook associated with esd_pipeline is the file esd_pipeline.ipynb. The main objective of esd_pipeline is to extract a small sample of size assigned to the variable data_sample_size from all data stored in the folder whose name is assigned to the variable experimentation_dir which contains the following elements.\n\nground_truth_data: It is a variable for the name of the csv file of ground truth data.\nestimated_data: It is a variable for the name of the csv file of estimated data.\nerror_data: It is a variable for the name of the csv file of error data.\nimages_dir: It is a variable for the name of the folder containing images.\n\nThe extracted datasets are stored in the folder whose name is made of sample+experimentation_dir. Its contents are organized like that of the folder indicated in the variable experimentation_dir.\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(experimentation_dir)\n  I --&gt; B(data_sample_size)\n  A --&gt; A1(images_dir)\n  A --&gt; A2(ground_truth_data)\n  A --&gt; A3(estimated_data)\n  A --&gt; A4(error_data)\n  A --&gt; C{esd_pipeline}\n  B --&gt; C\n  C --&gt; O[output]\n  O --&gt; D(sample+experimentation_dir)\n  D --&gt; D1(images_dir)\n  D --&gt; D2(ground_truth_data)\n  D --&gt; D3(estimated_data)\n  D --&gt; D4(error_data)",
    "crumbs": [
      "Data",
      "Data operations"
    ]
  },
  {
    "objectID": "data-operations.html#extract-transform-load-etl-pipeline-mandatory",
    "href": "data-operations.html#extract-transform-load-etl-pipeline-mandatory",
    "title": "Data operations",
    "section": "Extract Transform Load (etl) pipeline: Mandatory",
    "text": "Extract Transform Load (etl) pipeline: Mandatory\nThe notebook associated with etl_pipeline is the file etl_pipeline.ipynb. The main objective of this pipeline is to construct the file merged_data.csv. This file contains information on the instantaneous positions and velocities of the car sensors studied, the statistics of occurrences as well as the areas and locations associated with all classes of traffic scene objects. To produce the file merged_data.csv, the etl_pipeline requires two main information as input. The former is the folder whose name is assigned to the variable experimentation_dir. This folder must be generated by esd_pipeline since the running evops app container has all the privileges on it. It obviously contains the following elements.\n\nground_truth_data: It is a variable for the name of the csv file of ground truth data.\nestimated_data: It is a variable for the name of the csv file of estimated data.\nerror_data: It is a variable for the name of the csv file of error data.\nimages_dir: It is a variable for the name of the folder containing images.\n\nThe latter is a positive integer assigned to the variable batch_size which is the number of images to process at each iteration by the object detector to avoid a memory issue. The outputs are stored in the folder whose name is assigned to the variable experimentation_dir. This folder includes the following elements.\n\nsplit_images_without_detections: It is a folder in which all images from the folder whose name is assigned to the variable images_dir are partitioned. Images of the k-th partition are stored in the folder split_k.\nsplit_images_with_detections: It is a folder organized like split_images_without_detections but contains annotated images by bounding boxes associated with all detected object classes.\nsplit_images_detection_coordinates: It a folder organized like split_images_with_detections but instead of annotated images, it contains the coordinates of the related bounding boxes in csv files.\nextracted_detections.csv: It is a csv file which gathers the contents of all csv files stored in the folder split_images_detection_coordinates.\ntransformed_detections.csv: It is a csv file which results from the transformation of the csv file extracted_detections.csv in order to synthetize it and provide more information on detections in some additional variables.\nmerged_data.csv: It is the target file of etl_pipeline. This file contains not only variables from the file transformed_detections.csv and those from the file whose name is assigned to the variable error_data, but also the variables of velocity calculated using data from the files whose names are assigned to the variables ground_truth_data and estimated_data.\n\n\n\n\n\n\nflowchart LR\n I[Input] --&gt; A(experimentation_dir)\n I --&gt; B(batch_size)\n A --&gt; A1(images_dir)\n A --&gt; A2(ground_truth_data)\n A --&gt; A3(estimated_data)\n A --&gt; A4(error_data)\n A --&gt; C{etl_pipeline}\n B --&gt; C\n C --&gt; O[output]\n O --&gt; D(experimentation_dir)\n D --&gt; D3(split_images_without_detections)\n D --&gt; D2(split_images_with_detections)\n D --&gt; D1(split_images_detection_coordinates)\n D --&gt; D4(extracted_detections.csv)\n D --&gt; D5(transformed_detections.csv)\n D --&gt; D6(merged_data.csv)",
    "crumbs": [
      "Data",
      "Data operations"
    ]
  },
  {
    "objectID": "data-operations.html#outlier-data-extraction-ode-pipeline-optional",
    "href": "data-operations.html#outlier-data-extraction-ode-pipeline-optional",
    "title": "Data operations",
    "section": "Outlier Data Extraction (ode) pipeline: Optional",
    "text": "Outlier Data Extraction (ode) pipeline: Optional\nThe notebook associated with ode_pipeline is the file ode_pipeline.ipynb. The main objective of this pipeline is to extract all outliers associated a given variable. Before running the ode_pipeline, appropriate values must be assigned to the following input arguments.\n\ncoefficient_iqr: It is a variable for the real value used to calculate the bounds of an interval out of which data are considered as outliers.\nexperimentation_dir: It is a variable for the name of folder which contains both the csv file of errors whose name is assigned to the variable error_data and the folder of images whose name is assigned to the variable images_dir.\nresponse_var: It is a variable for the name of the considered column in the file referenced by the variable error_data.\n\nThe outputs are stored in the folder whose name is assigned to the variable experimentation_dir. In this directory, images and data associated with the outliers are stored in the sub folder whose name is assigned to the variable response_var of the folders outliers_images and outliers_data, respectively. The name of the csv file containing outliers data is made of outliers+response_var+csv.\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(experimentation_dir)\n  A --&gt; A1(error_data)\n  A --&gt; A2(images_dir)\n  I[Input] --&gt; B(response_var)\n  I[Input] --&gt; C(coefficient_iqr)\n  A --&gt; D{ode_pipeline}\n  B --&gt; D\n  C --&gt; D\n  D --&gt; O[output]\n  O --&gt; E(experimentation_dir)\n  E --&gt; F(outliers_images)\n  F --&gt; F1(response_var)\n  F1 --&gt; F2(images_files)\n  E --&gt; G(outliers_data)\n  G --&gt; H(response_var)\n  H --&gt; H1(outliers+response_var+csv)\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(experimentation_dir)\n  A --&gt; A1(error_data)\n  A --&gt; A2(images_dir)\n  I[Input] --&gt; B(response_var)\n  I[Input] --&gt; C(coefficient_iqr)\n  A --&gt; D{ode_pipeline}\n  B --&gt; D\n  C --&gt; D\n  D --&gt; O[output]\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  O[output] --&gt; E(experimentation_dir)\n  E --&gt; F(outliers_images)\n  F --&gt; F1(response_var)\n  F1 --&gt; F2(images_files)\n  E --&gt; G(outliers_data)\n  G --&gt; H(response_var)\n  H --&gt; H1(outliers+response_var+csv)",
    "crumbs": [
      "Data",
      "Data operations"
    ]
  },
  {
    "objectID": "data-operations.html#object-detection-statistics-ods-pipeline-recommended",
    "href": "data-operations.html#object-detection-statistics-ods-pipeline-recommended",
    "title": "Data operations",
    "section": "Object Detection Statistics (ods) pipeline: Recommended",
    "text": "Object Detection Statistics (ods) pipeline: Recommended\nThe notebook associated with ods_pipeline is the file ods_pipeline.ipynb. The main objective of this pipeline is to extract statistics on the total number of detections associated with each class of objects. These statistics along with the related bar chart are displayed in the notebook. The detection statistics are also stored in the csv file named detection_statistics.csv in the directory whose name is assigned to the input argument experimentation_dir. The ods_pipeline depends on the etl_pipeline since its execution requires that the csv file merged_data.csv be present in the directory referenced by the variable experimentation_dir. This is why the name merged_data.csv is assigned by default to the input argument merged_data.\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(experimentation_dir)\n  A --&gt; A1(merged_data)\n  A --&gt; B{ods_pipeline}\n  B --&gt; O[output]\n  O --&gt; C(experimentation_dir)\n  C --&gt; C1(detection_statistics.csv)",
    "crumbs": [
      "Data",
      "Data operations"
    ]
  },
  {
    "objectID": "data-operations.html#impute-transform-load-itl-pipeline-mandatory",
    "href": "data-operations.html#impute-transform-load-itl-pipeline-mandatory",
    "title": "Data operations",
    "section": "Impute Transform Load (itl) pipeline: Mandatory",
    "text": "Impute Transform Load (itl) pipeline: Mandatory\n\n\n\n\n\n\nImportant\n\n\n\nBefore running itl_pipeline, it is recommended to run ods_pipeline and explore the extracted detection statistics which are ordered from the most detected object class to the least detected object class. Then, define the number of object classes whose detection statistics are considered as large enough. That number is the value which must be assigned to the input variable nlargest_classes of the itl_pipeline.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn general, the file merged_data.csv generated by etl_pipeline contains non standardized data and a significant number of misssing values due to the absence of some object classes in certain parts of the traffic scene.\n\n\nThe notebook associated with itl_pipeline is the file itl_pipeline.ipynb. The objectives of itl_pipeline are to impute the missing values in the file merged_data.csv and to standardize the values of its variables in order to generated data ready to feed statistical learning models.\nTo run the itl_pipeline, a file named merged_data.csv generated by etl_pipeline must be present in the directories referenced by the variables experimentation_train_dir and experimentation_test_dir. In addition, a file named detection_statistics.csv which is generated by ods_pipeline must be present in the directories referenced by the variables experimentation_train_dir.\nBesides, the studied response variable name must be assigned to the input argument response_var while the number of most detected object classes (identified when exploring the output generated by ods_pipeline) must be assigned to the input argument nlargest_classes. Note that in order to use an optimal value of this parameter, it suffices to assign the value -1 to the variable nlargest_classes.\nFinally, the selected_classes parameter is used to indicate the names of selected object classes to focus on. Obviously, the input argument selected_classes takes precedence over the nlargest_classes parameter when it refers to a non-null value.\nOutputs are stored in the sub folder processed_data of the directory whose name is assigned to the variable experimentation_train_dir. In the directory processed_data, the sub folder whose name is the value assigned to the input argument response_var contains the four csv files conventionally named as follows.\n\nexperimentation_train_dir+train+response_var+response.csv: It is the file which contains the train response variable.\nexperimentation_train_dir+train+response_var+scaled_covariates.csv: It is the file which contains the train predictor variables.\nexperimentation_test_dir+test+response_var+response.csv: It is the file which contains the test response variable.\nexperimentation_test_dir+test+response_var+scaled_covariates.csv: It is the file which contains the test predictor variables.\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(experimentation_train_dir)\n  A --&gt; A1(merged_data.csv)\n  A --&gt; A2(detection_statistics.csv)\n  I[Input] --&gt; B(experimentation_test_dir)\n  B --&gt; B1(merged_data.csv)\n  I --&gt; D(nlargest_classes)\n  I --&gt; D2(selected_classes)\n  I --&gt; C(response_var)\n  A --&gt; E{itl_pipeline}\n  B --&gt; E\n  C --&gt; E\n  D --&gt; E\n  D2 --&gt; E\n  E --&gt; O[output]\n  O --&gt; F(experimentation_train_dir)\n  F --&gt; G(processed_data)\n  G --&gt; H(response_var)\n  H --&gt; H1(experimentation_train_dir+train+response_var+response.csv)\n  H --&gt; H2(experimentation_train_dir+train+response_var+scaled_covariates.csv)\n  H --&gt; H3(experimentation_test_dir+test+response_var+response.csv)\n  H --&gt; H4(experimentation_test_dir+test+response_var+scaled_covariates.csv)\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(experimentation_train_dir)\n  A --&gt; A1(merged_data.csv)\n  A --&gt; A2(detection_statistics.csv)\n  I[Input] --&gt; B(experimentation_test_dir)\n  B --&gt; B1(merged_data.csv)\n  I --&gt; D(nlargest_classes)\n  I --&gt; D2(selected_classes)\n  I --&gt; C(response_var)\n  A --&gt; E{itl_pipeline}\n  B --&gt; E\n  C --&gt; E\n  D --&gt; E\n  D2 --&gt; E\n  E --&gt; O[output]\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  O[output] --&gt; F(experimentation_train_dir)\n  F --&gt; G(processed_data)\n  G --&gt; H(response_var)\n  H --&gt; H1(experimentation_train_dir+train+response_var+response.csv)\n  H --&gt; H2(experimentation_train_dir+train+response_var+scaled_covariates.csv)\n  H --&gt; H3(experimentation_test_dir+test+response_var+response.csv)\n  H --&gt; H4(experimentation_test_dir+test+response_var+scaled_covariates.csv)",
    "crumbs": [
      "Data",
      "Data operations"
    ]
  },
  {
    "objectID": "statistical-models.html#arithmetic-mixture-of-gev-laws",
    "href": "statistical-models.html#arithmetic-mixture-of-gev-laws",
    "title": "Statistical models",
    "section": "",
    "text": "The extreme value model to estimate is the arithmetic mixture of generalized extreme value (GEV) distribution \\(G_{\\mathbb{S}}\\) defined by\n\\[\nG_{\\mathbb{S}}(x;\\omega) = \\sum_{j = 1}^{p} \\omega_j\\, G_{\\mathbb{S}, j}(x),\n\\] where \\(x \\in \\mathbb{R};\\) \\(\\omega = \\left(\\omega_1, \\cdots, \\omega_p \\right);\\) \\(0 &lt; \\omega_j \\leq 1;\\) \\(\\sum_{j = 1}^{p}\\omega_j = 1\\) and \\(G_{\\mathbb{S}, j}\\) is a GEV distribution with parameters \\(\\left(\\gamma_{\\mathbb{S}, j},\\sigma_{\\mathbb{S}, j},\\mu_{\\mathbb{S}, j}\\right)\\in \\mathbb{R}^3.\\)\n\n\n\nLet \\(y = (y_1, \\cdots, y_q)\\in \\mathbb{R}^q\\) be a vector of \\(q\\) predictor variables. The extreme value model to estimate is the arithmetic mixture of generalized extreme value (GEV) distribution \\(G_{\\mathbb{S}}(\\cdot|y)\\) defined by\n\\[\nG_{\\mathbb{S}}(x;\\omega|y) = \\sum_{j = 1}^{p} \\omega_j\\, G_{\\mathbb{S}, j}(x|y),\n\\] where \\(x \\in \\mathbb{R};\\) \\(\\omega = \\left(\\omega_1, \\cdots, \\omega_p \\right);\\) \\(0 &lt; \\omega_j \\leq 1;\\) \\(\\sum_{j = 1}^{p}\\omega_j = 1\\) and \\(G_{\\mathbb{S}, j}(\\cdot|y)\\) is a GEV distribution with parameters \\(\\left(\\gamma_{\\mathbb{S}, j},\\sigma_{\\mathbb{S}, j}(y),\\mu_{\\mathbb{S}, j}(y)\\right)\\in \\mathbb{R}^3.\\)\nwith\n\\[\\mu_{\\mathbb{S}, j}(y) = \\mu_{\\mathbb{S},j, 0} + \\sum_{\\ell = 1}^{q} \\mu_{\\mathbb{S},j,\\ell}\\, y_{\\ell}\\]\nand\n\\[\\sigma_{\\mathbb{S}, j}(y) = \\sigma_{\\mathbb{S},j, 0} + \\sum_{\\ell = 1}^{q} \\sigma_{\\mathbb{S},j,\\ell}\\, y_{\\ell}.\\]",
    "crumbs": [
      "Model",
      "Statistical models"
    ]
  },
  {
    "objectID": "statistical-models.html#geometric-mixtures-of-gev-laws",
    "href": "statistical-models.html#geometric-mixtures-of-gev-laws",
    "title": "Statistical models",
    "section": "Geometric mixtures of gev laws",
    "text": "Geometric mixtures of gev laws\nThe extreme value model to estimate is the geometric mixture of generalized extreme value (GEV) distribution \\(G_{\\mathbb{P}}\\) defined by \\[\nG_{\\mathbb{P}}(x;\\omega) = \\prod_{j = 1}^{p} G^{\\omega_j}_{\\mathbb{P}, j}(x),\n\\] where \\(x \\in \\mathbb{R};\\) \\(\\omega = \\left(\\omega_1, \\cdots, \\omega_p \\right);\\) \\(0 &lt; \\omega_j \\leq 1;\\) \\(\\sum_{j = 1}^{p}\\omega_j = 1\\) and \\(G_{\\mathbb{P}, j}\\) is a GEV distribution with parameters \\(\\left(\\gamma_{\\mathbb{P}, j},\\sigma_{\\mathbb{P}, j},\\mu_{\\mathbb{P}, j}\\right)\\in \\mathbb{R}^3.\\)",
    "crumbs": [
      "Model",
      "Statistical models"
    ]
  },
  {
    "objectID": "model-operations.html#statistical-learning-analysis-sla-pipeline-mandatory",
    "href": "model-operations.html#statistical-learning-analysis-sla-pipeline-mandatory",
    "title": "Model operations",
    "section": "",
    "text": "Note\n\n\n\nRerunning this pipeline with eventually modified input arguments on the same dataset of a given experimentation will overwrite all results generated during the previous run.\n\n\nThe notebook associated with sla_pipeline is the file sla_pipeline.ipynb. The main objective of this pipeline is to estimate the parameters of (stationary and non stationary) arithmetic mixture of gev models based on train datasets and to exploit the fitted model to predict return levels associated with given return period conditional on test predictor variables. Here, the train and test datasets must be those generated by itl_pipeline.\nTo run sla_pipeline, the studied response variable name must be assigned to the input argument response_var. In addition, the folder names containing raw train and test datasets must be assigned to the input arguments experimentation_train_dir and experimentation_test_dir. These three input arguments must match those used in itl_pipeline to create the considered train and test datasets. Note that the estimated models are automatically stored for future use since sla_pipeline can take a while to complete. Thus, a Boolean value must be assigned to the input argument use_pre_trained_models to indicate whether to load and exploit pre-trained models provided that they exist.\nAll numerical and graphical outputs are displayed in the notebook. Some numerical outputs are stored in the sub folder processed_data of the directory whose name is assigned to the variable experimentation_train_dir. In the directory processed_data, the sub folder whose name is the value assigned to the input argument response_var includes the csv files conventionally named as follows.\n\nexperimentation_train_dir+response_var+stationary_model_estimates.csv: It is the file which contains the estimated parameters of the stationary arithmetic gev mixture model fitted using only the train response variable.\nexperimentation_train_dir+response_var+non_stationary_model_estimates.csv: It is the file which contains the estimated parameters of the non stationary arithmetic gev mixture model fitted using both the train response variable and train predictor variables.\nexperimentation_train_dir+response_var+arithmetic_gev_mixture_model.rds: It is the file which contains all useful information about the estimated stationary arithmetic gev mixture model. This file, if it exists, is loaded for model exploitation when the parameter use_pre_trained_models is set to TRUE.\nexperimentation_train_dir+response_var+arithmetic_ns_gev_mixture_model.rds: It is the file which contains all useful information about the estimated non stationary arithmetic gev mixture model. This file, if it exists, is loaded for model exploitation when the parameter use_pre_trained_models is set to TRUE.\nexperimentation_train_dir+response_var+sample.csv: It is the file which contains a sample of the response variable generated using the fitted stationary arithmetic gev mixture model. The sample size can be modified as per needs. Its default value is equal to the number of observations in the train datasets.\nexperimentation_train_dir+response_var+importance.csv: It is the file which contains the contribution or importance of the predictor variables on the expected exceedance above the maximum value of the train response variable.\nexperimentation_train_dir+experimentation_test_dir+response_var+return_periods.csv: It is the file which contains return periods of observations in the test response variable conditional on their respective test predictor variables.\nexperimentation_train_dir+experimentation_test_dir+response_var+return_levels.csv: It is the file which contains return levels of the response variable conditional on the observed test predictor variables. The common return period associated with these return levels can be modified as per needs. Its default value is equal to the number of observations in the train datasets.\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(experimentation_train_dir)\n  I[Input] --&gt; B(experimentation_test_dir)\n  I[Input] --&gt; C(response_var)\n  I[Input] --&gt; C1(use_pre_trained_models)\n  A --&gt; D{sla_pipeline}\n  B --&gt; D\n  C --&gt; D\n  C1 --&gt; D\n  D --&gt; O[output]\n  O --&gt; E(experimentation_train_dir)\n  E --&gt; F(processed_data)\n  F --&gt; G(response_var)\n  G --&gt; G5(experimentation_train_dir+response_var+stationary_model_estimates.csv)\n  G --&gt; G6(experimentation_train_dir+response_var+non_stationary_model_estimates.csv)\n  G --&gt; G9(experimentation_train_dir+response_var+arithmetic_gev_mixture_model.rds)\n  G --&gt; G10(experimentation_train_dir+response_var+arithmetic_ns_gev_mixture_model.rds)\n  G --&gt; G3(experimentation_train_dir+response_var+sample.csv)\n  G --&gt; G4(experimentation_train_dir+response_var+importance.csv)\n  G --&gt; G7(experimentation_train_dir+experimentation_test_dir+response_var+return_periods.csv)\n  G --&gt; G8(experimentation_train_dir+experimentation_test_dir+response_var+return_levels.csv)\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(experimentation_train_dir)\n  I[Input] --&gt; B(experimentation_test_dir)\n  I[Input] --&gt; C(response_var)\n  I[Input] --&gt; C1(use_pre_trained_models)\n  A --&gt; D{sla_pipeline}\n  B --&gt; D\n  C --&gt; D\n  C1 --&gt; D\n  D --&gt; O[output]\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n  O[output] --&gt; E(experimentation_train_dir)\n  E --&gt; F(processed_data)\n  F --&gt; G(response_var)\n  G --&gt; G5(experimentation_train_dir+response_var+stationary_model_estimates.csv)\n  G --&gt; G6(experimentation_train_dir+response_var+non_stationary_model_estimates.csv)\n  G --&gt; G9(experimentation_train_dir+response_var+arithmetic_gev_mixture_model.rds)\n  G --&gt; G10(experimentation_train_dir+response_var+arithmetic_ns_gev_mixture_model.rds)\n  G --&gt; G3(experimentation_train_dir+response_var+sample.csv)\n  G --&gt; G4(experimentation_train_dir+response_var+importance.csv)\n  G --&gt; G7(experimentation_train_dir+experimentation_test_dir+response_var+return_periods.csv)\n  G --&gt; G8(experimentation_train_dir+experimentation_test_dir+response_var+return_levels.csv)",
    "crumbs": [
      "Model",
      "Model operations"
    ]
  },
  {
    "objectID": "model-operations.html#extreme-value-analysis-eva-pipeline-independent",
    "href": "model-operations.html#extreme-value-analysis-eva-pipeline-independent",
    "title": "Model operations",
    "section": "Extreme Value Analysis (eva) pipeline: Independent",
    "text": "Extreme Value Analysis (eva) pipeline: Independent\nThere are currently four notebooks associated with eva_pipeline, namely eva_pipeline_normal.ipynb, eva_pipeline_gamma.ipynb, eva_pipeline_gev.ipynb and eva_pipeline.ipynb. The first three notebooks are intended to test eva_pipeline on simulated data (see Section 2.1 for more details) whereas the last notebook is intended to apply eva_pipeline on custom data (see Section 2.2 for more details).\n\nIllustration on simulated datasets\n\n\n\n\n\n\nNote\n\n\n\nRerunning the pipelines of this section with modified input arguments will overwrite all results generated during the previous run.\n\n\n\nNormal Distribution\nThe notebook associated with eva_pipeline for data from a normal distribution is the file eva_pipeline_normal.ipynb. The main objective of this pipeline is to estimate the parameters of stationary geometric mixture of gev models based on simulated data from a normal probability distribution and to exploit the fitted model to predict return levels and expected exceedances associated with given return periods. In addition, the true return levels of the underlying normal law are provided for comparison.\nThe eva_pipeline for normal distribution requires three main information as input. The first is the number of normal data to simulate. This number must be assigned to the input argument n. The second and third are the mean and standard deviation parameters of the normal law to use. These two quantities must be assigned to the input arguments mean and sd, respectively. Note that the estimated models are automatically stored for future use since eva_pipeline can take a while to complete. Thus, a Boolean value must be assigned to the input argument use_pre_trained_models to indicate whether to load and exploit pre-trained models provided that they exist.\nAll numerical and graphical outputs are displayed in the notebook. Some numerical outputs are stored in the normal_distribution_results directory as csv files conventionally named as follows.\n\nsample_normal_data.csv: It is the file which contains the simulated normal data using the provided input arguments.\ngev_mixture_model_estimates.csv: It is the file which contains the estimated parameters of the stationary geometric gev mixture model fitted using the simulated normal data.\nsample_normal_data_simulations_gev_mixture_models.rds: It is the file which contains all useful information about the estimated stationary geometric gev mixture model. This file, if it exists, is loaded for model exploitation when the parameter use_pre_trained_models is set to TRUE.\nreturn_levels.csv: It is the file which contains estimated and true return levels of the normal law. The return periods associated with these return levels range from the number of observations in the simulated normal data to one million times that number. The used default values of return periods can be modified as per needs.\nexpected_exceedance.csv: It is the file which contains the estimated expected exceedances of a given threshold related to the input normal law for all gev models in the fitted geometric gev mixture model. The default value of that threshold is the maximum value of the simulated normal data. This value can be modified as per needs.\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(n)\n  I[Input] --&gt; B(mean)\n  I[Input] --&gt; C(sd)\n  I[Input] --&gt; C1(use_pre_trained_models)\n  A --&gt; D{eva_pipeline}\n  B --&gt; D\n  C --&gt; D\n  C1 --&gt; D\n  D --&gt; O[output]\n  O --&gt; E(normal_distribution_results)\n  E --&gt; E1(sample_normal_data.csv)\n  E --&gt; E2(gev_mixture_model_estimates.csv)\n  E --&gt; E5(sample_normal_data_simulations_gev_mixture_models.rds)\n  E --&gt; E3(return_levels.csv)\n  E --&gt; E4(expected_exceedance.csv)\n\n\n\n\n\n\n\n\nGamma Distribution\nThe notebook associated with eva_pipeline for data from a gamma distribution is the file eva_pipeline_gamma.ipynb. The main objective of this pipeline is to estimate the parameters of stationary geometric mixture of gev models based on simulated data from a gamma probability distribution and to exploit the fitted model to predict return levels and expected exceedances associated with given return periods. In addition, the true return levels of the underlying gamma law are provided for comparison.\nThe eva_pipeline for gamma distribution requires three main information as input. The first is the number of gamma data to simulate. This number must be assigned to the input argument n. The second and third are the shape and scale parameters of the gamma law to use. These two quantities must be assigned to the input arguments shape and scale, respectively. Note that the estimated models are automatically stored for future use since eva_pipeline can take a while to complete. Thus, a Boolean value must be assigned to the input argument use_pre_trained_models to indicate whether to load and exploit pre-trained models provided that they exist.\nAll numerical and graphical outputs are displayed in the notebook. Some numerical outputs are stored in the gamma_distribution_results directory as csv files conventionally named as follows.\n\nsample_gamma_data.csv: It is the file which contains the simulated gamma data using the provided input arguments.\ngev_mixture_model_estimates.csv: It is the file which contains the estimated parameters of the stationary geometric gev mixture model fitted using the simulated gamma data.\nsample_gamma_data_simulations_gev_mixture_models.rds: It is the file which contains all useful information about the estimated stationary geometric gev mixture model. This file, if it exists, is loaded for model exploitation when the parameter use_pre_trained_models is set to TRUE.\nreturn_levels.csv: It is the file which contains estimated and true return levels of the gamma law. The return periods associated with these return levels range from the number of observations in the simulated gamma data to one million times that number. The used default values of return periods can be modified as per needs.\nexpected_exceedance.csv: It is the file which contains the estimated expected exceedances of a given threshold related to the input gamma law for all gev models in the fitted geometric gev mixture model. The default value of that threshold is the maximum value of the simulated gamma data. This value can be modified as per needs.\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(n)\n  I[Input] --&gt; B(shape)\n  I[Input] --&gt; C(scale)\n  I[Input] --&gt; C1(use_pre_trained_models)\n  A --&gt; D{eva_pipeline}\n  B --&gt; D\n  C --&gt; D\n  C1 --&gt; D\n  D --&gt; O[output]\n  O --&gt; E(gamma_distribution_results)\n  E --&gt; E1(sample_gamma_data.csv)\n  E --&gt; E2(gev_mixture_model_estimates.csv)\n  E --&gt; E5(sample_gamma_data_simulations_gev_mixture_models.rds)\n  E --&gt; E3(return_levels.csv)\n  E --&gt; E4(expected_exceedance.csv)\n\n\n\n\n\n\n\n\nGEV Distribution\nThe notebook associated with eva_pipeline for data from a gev distribution is the file eva_pipeline_gev.ipynb. The main objective of this pipeline is to estimate the parameters of stationary geometric mixture of gev models based on simulated data from a gev probability distribution and to exploit the fitted model to predict return levels and expected exceedances associated with given return periods. In addition, the true return levels of the underlying gev law are provided for comparison.\nThe eva_pipeline for gev distribution requires four main information as input. The first is the number of gev data to simulate. This number must be assigned to the input argument n. The second, third and fourth are the shape, scale and location parameters of the gev law to use. These three quantities must be assigned to the input arguments shape, scale and loc, respectively. Note that the estimated models are automatically stored for future use since eva_pipeline can take a while to complete. Thus, a Boolean value must be assigned to the input argument use_pre_trained_models to indicate whether to load and exploit pre-trained models provided that they exist.\nAll numerical and graphical outputs are displayed in the notebook. Some numerical outputs are stored in the gev_distribution_results directory as csv files conventionally named as follows.\n\nsample_gev_data.csv: It is the file which contains the simulated gev data using the provided input arguments.\ngev_mixture_model_estimates.csv: It is the file which contains the estimated parameters of the stationary geometric gev mixture model fitted using the simulated gev data.\nsample_gev_data_simulations_gev_mixture_models.rds: It is the file which contains all useful information about the estimated stationary geometric gev mixture model. This file, if it exists, is loaded for model exploitation when the parameter use_pre_trained_models is set to TRUE.\nreturn_levels.csv: It is the file which contains estimated and true return levels of the gev law. The return periods associated with these return levels range from the number of observations in the simulated gev data to one million times that number. The used default values of return periods can be modified as per needs.\nexpected_exceedance.csv: It is the file which contains the estimated expected exceedances of a given threshold related to the input gev law for all gev models in the fitted geometric gev mixture model. The default value of that threshold is the maximum value of the simulated gev data. This value can be modified as per needs.\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(n)\n  I[Input] --&gt; B1(shape)\n  I[Input] --&gt; B2(scale)\n  I[Input] --&gt; B3(loc)\n  I[Input] --&gt; C1(use_pre_trained_models)\n  A --&gt; D{eva_pipeline}\n  B1 --&gt; D\n  B2 --&gt; D\n  B3 --&gt; D\n  C1 --&gt; D\n  D --&gt; O[output]\n  O --&gt; E(gev_distribution_results)\n  E --&gt; E1(sample_gev_data.csv)\n  E --&gt; E2(gev_mixture_model_estimates.csv)\n  E --&gt; E5(sample_gev_data_simulations_gev_mixture_models.rds)\n  E --&gt; E3(return_levels.csv)\n  E --&gt; E4(expected_exceedance.csv)\n\n\n\n\n\n\n\n\n\nApplication on real dataset\n\n\n\n\n\n\nNote\n\n\n\nRerunning this pipeline with modified input arguments on a given uploaded dataset will overwrite all results generated during the previous run.\n\n\nThe notebook associated with eva_pipeline is the file eva_pipeline.ipynb. The main objective of this pipeline is to estimate the parameters of (stationary) geometric mixture of gev models based on train data and to exploit the fitted model to predict return levels and expected exceedances associated with given return periods.\nThe eva_pipeline requires two main information as input. The former is the csv file whose name is assigned to the input argument csv_file_name. This csv file must be uploaded in the workspace directory of the container running the evops app. The latter concerns the column name of the variable to consider in the file referenced by the argument csv_file_name. This column name must be assigned to the input argument variable_name. Note that the estimated models are automatically stored for future use since eva_pipeline can take a while to complete. Thus, a Boolean value must be assigned to the input argument use_pre_trained_models to indicate whether to load and exploit pre-trained models provided that they exist.\nAll numerical and graphical outputs are displayed in the notebook. Some numerical outputs are stored in the workspace directory of the container running the evops app as csv files conventionally named as follows.\n\ncsv_file_name+variable_name+gev_mixture_model_estimates.csv: It is the file which contains the estimated parameters of the stationary geometric gev mixture model fitted using the specified train data.\ncsv_file_name+variable_name+gev_mixture_models.rds: It is the file which contains all useful information about the estimated stationary geometric gev mixture model. This file, if it exists, is loaded for model exploitation when the parameter use_pre_trained_models is set to TRUE.\ncsv_file_name+variable_name+return_levels.csv: It is the file which contains return levels of the fitted variable. The return periods associated with these return levels range from the number of observations in the train data to one million times that number. The used default values of return periods can be modified as per needs.\ncsv_file_name+variable_name+expected_exceedance.csv: It is the file which contains the expected exceedances of a given threshold related to the fitted variable for all gev models in the fitted geometric gev mixture model. The default value of that threshold is the maximum value of the train data. This value can be modified as per needs.\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(csv_file_name)\n  I[Input] --&gt; B(variable_name)\n  I[Input] --&gt; C1(use_pre_trained_models)\n  A --&gt; C{eva_pipeline}\n  B --&gt; C\n  C1 --&gt; C\n  C --&gt; O[output]\n  O --&gt; D(workspace)\n  D --&gt; D1(csv_file_name+variable_name+gev_mixture_model_estimates.csv)\n  D --&gt; D4(csv_file_name+variable_name+gev_mixture_models.rds)\n  D --&gt; D2(csv_file_name+variable_name+return_levels.csv)\n  D --&gt; D3(csv_file_name+variable_name+expected_exceedance.csv)",
    "crumbs": [
      "Model",
      "Model operations"
    ]
  },
  {
    "objectID": "manipulate-evops.html#reset-all-pipeline-cells",
    "href": "manipulate-evops.html#reset-all-pipeline-cells",
    "title": "Manipulate evops app",
    "section": "Reset all pipeline cells",
    "text": "Reset all pipeline cells\nAll cells, even running ones, associated with a pipeline notebook can be reset as follows.\n\nClick on the considered open pipeline notebook tab in the main panel.\nClick on the Kernel tab from the Jupyter menu.\nSelect Restart Kernel in the scrolling menu. This will stop and clear all outputs. The notebook can then rerun in a clean environment.\n\n\n\n\nlisting_045",
    "crumbs": [
      "Manipulate",
      "Manipulate evops app"
    ]
  },
  {
    "objectID": "manipulate-evops.html#delete-saved-pipeline-outputs",
    "href": "manipulate-evops.html#delete-saved-pipeline-outputs",
    "title": "Manipulate evops app",
    "section": "Delete saved pipeline outputs",
    "text": "Delete saved pipeline outputs\nTo delete a folder or a csv file generated and saved by a pipeline in the workspace directory of the container running evops app, one can follow the instructions below.\n\nClick to hide or show the file browser.\nClick to display the folders of available pipelines and directories.\nDouble-click on the workspace directory to display its contents.\nBrowse to find and right-click on the element to delete.\nSelect Delete in the scrolling menu and confirm the action.\n\n\n\n\nlisting_046\n\n\n\n\n\nlisting_047",
    "crumbs": [
      "Manipulate",
      "Manipulate evops app"
    ]
  },
  {
    "objectID": "manipulate-evops.html#delete-saved-a-pipeline-output",
    "href": "manipulate-evops.html#delete-saved-a-pipeline-output",
    "title": "Manipulate evops app",
    "section": "Delete saved a pipeline output",
    "text": "Delete saved a pipeline output\nTo delete a folder or csv file generated and saved by a pipeline in the workspace directory of the container running evops app, one can follow the instructions below.\n\nClick to hide or show the file browser.\nClick to display the folders of available pipelines and directories.\nDouble-click on the workspace directory to display its contents.\nBrowse to find and right-click on the element to delete.\nSelect Delete in the scrolling menu and confirm the action.\n\n\n\n\nlisting_046\n\n\n\n\n\nlisting_047",
    "crumbs": [
      "Manipulate",
      "Manipulate evops app"
    ]
  },
  {
    "objectID": "data-operations.html#extract-sample-data-esd-pipeline-mandatory",
    "href": "data-operations.html#extract-sample-data-esd-pipeline-mandatory",
    "title": "Data operations",
    "section": "Extract Sample Data (esd) pipeline: Mandatory",
    "text": "Extract Sample Data (esd) pipeline: Mandatory\nThe notebook associated with esd_pipeline is the file esd_pipeline.ipynb. The main objective of esd_pipeline is to extract or make a copy of a sample of size assigned to the variable data_sample_size from all data stored in the folder whose name is assigned to the variable experimentation_dir which contains the elements listed below.\n\nground_truth_data: It is a variable for the name of the csv file of ground truth data.\nestimated_data: It is a variable for the name of the csv file of estimated data.\nerror_data: It is a variable for the name of the csv file of error data.\nimages_dir: It is a variable for the name of the folder containing images.\n\nNote that in order to make a copy of all data in the folder referenced by the variable experimentation_dir, it suffices to assign the value -1 to the variable data_sample_size. The extracted datasets are stored in the folder whose name is made of sample+experimentation_dir. Its contents are organized like that of the folder indicated in the variable experimentation_dir.\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(experimentation_dir)\n  I --&gt; B(data_sample_size)\n  A --&gt; A1(images_dir)\n  A --&gt; A2(ground_truth_data)\n  A --&gt; A3(estimated_data)\n  A --&gt; A4(error_data)\n  A --&gt; C{esd_pipeline}\n  B --&gt; C\n  C --&gt; O[output]\n  O --&gt; D(sample+experimentation_dir)\n  D --&gt; D1(images_dir)\n  D --&gt; D2(ground_truth_data)\n  D --&gt; D3(estimated_data)\n  D --&gt; D4(error_data)",
    "crumbs": [
      "Data",
      "Data operations"
    ]
  },
  {
    "objectID": "data-operations.html#comparative-data-analysis-cda-pipeline-optional",
    "href": "data-operations.html#comparative-data-analysis-cda-pipeline-optional",
    "title": "Data operations",
    "section": "Comparative data analysis (cda) pipeline: Optional",
    "text": "Comparative data analysis (cda) pipeline: Optional\nThe notebook associated with cda_pipeline is the file cda_pipeline.ipynb. The main objective of this pipeline is to provide some comparative graphics of response variables and predictor variables associated with both train and test datasets.\nTo run the cda_pipeline, a file named merged_data.csv generated by etl_pipeline must be present in the directories referenced by the variables experimentation_train_dir and experimentation_test_dir.\nAll outputs are displayed in the notebook and can be exported for reporting. Numerical outputs are stored in the directory whose name is assigned to the variable experimentation_train_dir. This folder contains the five csv files whose names are listed below.\n\nlongitudinal_error_summary_statistics.csv: It is the file which contains summary statistics of the variable longitudinal_error associated with both train and test datasets.\nlateral_error_summary_statistics.csv: It is the file which contains summary statistics of the variable lateral_error associated with both train and test datasets.\nvelocity_summary_statistics.csv: It is the file which contains summary statistics of the variable velocity associated with both train and test datasets.\nobject_summary_statistics.csv: It is the file which contains summary statistics of the variable object associated with both train and test datasets. This variable stands for the number of objects per image.\narea_summary_statistics.csv: It is the file which contains summary statistics of the variable area associated with both train and test datasets. This variable stands for the normalized total area of objects per image.\n\n\n\n\n\n\nflowchart LR\n  I[Input] --&gt; A(experimentation_train_dir)\n  A --&gt; A1(merged_data.csv)\n  I[Input] --&gt; B(experimentation_test_dir)\n  B --&gt; B1(merged_data.csv)\n  A --&gt; E{cda_pipeline}\n  B --&gt; E\n  E --&gt; O[output]\n  O --&gt; H(experimentation_train_dir)\n  H --&gt; H1(longitudinal_error_summary_statistics.csv)\n  H --&gt; H2(lateral_error_summary_statistics.csv)\n  H --&gt; H3(velocity_summary_statistics.csv)\n  H --&gt; H4(object_summary_statistics.csv)\n  H --&gt; H5(area_summary_statistics.csv)",
    "crumbs": [
      "Data",
      "Data operations"
    ]
  },
  {
    "objectID": "setup-docker.html#install-image-tools",
    "href": "setup-docker.html#install-image-tools",
    "title": "Setup docker desktop",
    "section": "Install Image-Tools",
    "text": "Install Image-Tools\nTo save a docker image as a .tar archive for sharing, the docker save command can be used. An archived Docker image can be loaded using the docker load command. To learn more about using these commands in a terminal, interested readers can visit the following web page https://docs.docker.com/desktop/backup-and-restore/. To perform these tasks in Docker Desktop, an extension called Image-Tools must be installed. This installation can be done through the three steps listed below.\n\nClick on Extensions in the left sidebar menu of Docker Desktop.\nType image-tools in the search input box.\nClick on the Install button associated with the found result to start the installation process.\n\n\n\n\nlisting_048",
    "crumbs": [
      "Install",
      "Setup docker desktop"
    ]
  },
  {
    "objectID": "setup-evops.html#save-evops-app-image",
    "href": "setup-evops.html#save-evops-app-image",
    "title": "Setup evops app",
    "section": "Save evops app image",
    "text": "Save evops app image\n\n\n\n\n\n\nImportant\n\n\n\nBefore saving for the first time the docker image of the evops app, create a folder in your computer which is dedicated to the backup of evops app image in the host computer. To illustrate, we have created a folder called my_images at the location:\nC:\\Users\\HEWLETT-PACKARD\\Documents\\workdir\\my_images\n\n\n\nFrom Docker Desktop\nTo save a docker image as a .tar archive for sharing, one can proceed as follows in Docker Desktop on which the Image-Tools extension is installed.\n\nClick on Image-Tools in the left sidebar menu of Docker Desktop.\nClick on the Download icon of docker image to save. Note that the names of available docker images are located in the main panel under the RepoTags column. For instance, dkengne/evops:latest is the name of the latest version of evops app docker image.\nBrowse in the host computer to select the name of the folder in which to save the considered docker image.\nClick on the Sélectionner un dossier button to start the backup process.\n\n\n\n\nlisting_049\n\n\n\n\n\nlisting_050\n\n\n\n\nFrom Windows Terminal\nTo save a docker image as a .tar archive for sharing, one can proceed as described below in Windows Terminal after starting Docker Desktop. This two-step procedure exploits the docker save command whose use is explained on the following web page https://docs.docker.com/reference/cli/docker/image/save/.\n\nOpen a Windows Terminal. Copy and paste the docker command below in the open Terminal and hit the Enter key on the keyboard to display the list of available docker images. Note that the names of the found docker images and the related versions are respectively listed under the REPOSITORY and the TAG columns of the output.\n\ndocker images\n\nCopy and paste the command below in the open Terminal and hit the Enter key on the keyboard to start the backup process. This command is made up of three parts. The first part is docker save. The second part specifies the argument of the output option which is the path to the .tar archive to create. The third part indicates the name and the version of the docker image to save in the form REPOSITORY:TAG. It follows that before using the command below, the location C:\\Users\\HEWLETT-PACKARD\\Documents\\workdir\\my_images must be replaced with a valid location in your computer.\n\ndocker save --output C:\\Users\\HEWLETT-PACKARD\\Documents\\workdir\\my_images\\dkengne_evops_latest.tar dkengne/evops:latest\n\n\n\nlisting_053",
    "crumbs": [
      "Install",
      "Setup evops app"
    ]
  },
  {
    "objectID": "setup-evops.html#load-evops-app-image",
    "href": "setup-evops.html#load-evops-app-image",
    "title": "Setup evops app",
    "section": "Load evops app image",
    "text": "Load evops app image\n\nFrom Docker Desktop\nAn archived Docker image can be loaded into Docker Desktop that has the Image-Tools extension installed via the four steps listed below.\n\nClick on Image-Tools in the left sidebar menu of Docker Desktop.\nClick on the Import button in the main panel of Docker Desktop.\nBrowse in the host computer to select the name of the docker image to load. For instance, the latest version of evops app docker image is archived with the name dkengne_evops_latest.tar.\nClick on the Ouvrir button to start the backup restoration process.\n\n\n\n\nlisting_051\n\n\n\n\n\nlisting_052\n\n\n\n\nFrom Windows Terminal\nTo load an archived docker image, one can proceed as described below in Windows Terminal after starting Docker Desktop. This two-step procedure exploits the docker load command whose use is explained on the following web page https://docs.docker.com/reference/cli/docker/image/load/.\n\nCopy and paste the command below in the open Terminal and hit the Enter key on the keyboard to start the backup restoration process. This command is made up of two parts. The first part is docker load. The second part specifies the argument of the input option which is the path to the .tar archive to load. It follows that before using the command below, the file path C:\\Users\\HEWLETT-PACKARD\\Documents\\workdir\\my_images\\dkengne_evops_latest.tar must be replaced with an appropriate file path in your computer.\n\ndocker load --input C:\\Users\\HEWLETT-PACKARD\\Documents\\workdir\\my_images\\dkengne_evops_latest.tar\n\nCopy and paste the docker command below in the open Terminal and hit the Enter key on the keyboard to check whether the list of available docker images includes the one just loaded. Note that the names of the found docker images and the related versions are respectively listed under the REPOSITORY and the TAG columns of the output.\n\ndocker images\n\n\n\nlisting_054",
    "crumbs": [
      "Install",
      "Setup evops app"
    ]
  }
]